{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T06:22:59.294453Z","iopub.execute_input":"2023-11-20T06:22:59.295641Z","iopub.status.idle":"2023-11-20T06:22:59.307201Z","shell.execute_reply.started":"2023-11-20T06:22:59.295588Z","shell.execute_reply":"2023-11-20T06:22:59.306054Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"#Instal new version of Tensorflow\n#!pip3 install --upgrade tensorflow==2.12.0","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.349310Z","iopub.execute_input":"2023-11-20T06:22:59.349754Z","iopub.status.idle":"2023-11-20T06:22:59.355082Z","shell.execute_reply.started":"2023-11-20T06:22:59.349717Z","shell.execute_reply":"2023-11-20T06:22:59.353768Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"import random\nfrom datetime import datetime\nimport time\nimport resource\nimport pickle\nimport os\nimport pdb\n\n\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom scipy.interpolate import pchip_interpolate\n\nimport tensorflow as tf\n#import tensorflow.keras\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout\n#In old version of Tensorflow 2.8.0\n#from tensorflow.keras.optimizers import Adam\n#In new version of Tensorflow 2.12.0, please use legacy to import Adam\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.regularizers import l2\nimport tensorflow.keras.backend as K\n#from keras import backend as K\n\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom IPython.display import clear_output, display, HTML\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# originally built on old TensorFlow and Keras which didn't support eager execution\ntf.compat.v1.disable_eager_execution()\n#tf.compat.v1.disable_v2_behavior()\n#tf.compat.v1.enable_eager_execution()\n\n# set seeds for reproducibility\n# np.random.uniform(0,10000) 4465\nrandom.seed(4465)\nnp.random.seed(4465)\ntf.random.set_seed(4465)\n\nprint(\"TensorFlow %s\" % tf.__version__)\nprint(\"Keras %s\" % keras.__version__)\nprint(\"plotly %s\" % plotly.__version__)\nprint(\"pandas %s\" % pd.__version__)\nprint(\"numpy %s\" % np.__version__)\n\n# If model save directory isn't made yet, make it\nif not os.path.exists('model_output'):\n    os.makedirs('model_output')\nif not os.path.exists('model_output/trading'):\n    os.makedirs('model_output/trading')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.364227Z","iopub.execute_input":"2023-11-20T06:22:59.364904Z","iopub.status.idle":"2023-11-20T06:22:59.379806Z","shell.execute_reply.started":"2023-11-20T06:22:59.364868Z","shell.execute_reply":"2023-11-20T06:22:59.378746Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"TensorFlow 2.12.0\nKeras 2.12.0\nplotly 5.15.0\npandas 2.0.3\nnumpy 1.23.5\n","output_type":"stream"}]},{"cell_type":"code","source":"def sizeof_fmt(num, suffix='B'):\n    \"\"\"given memory as int format as memory units eg KB\"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Y', suffix)\n\ndef memusage():\n    \"\"\"print memory usage\"\"\"\n    return sizeof_fmt(int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n\nmemusage()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.402871Z","iopub.execute_input":"2023-11-20T06:22:59.403704Z","iopub.status.idle":"2023-11-20T06:22:59.413450Z","shell.execute_reply.started":"2023-11-20T06:22:59.403651Z","shell.execute_reply":"2023-11-20T06:22:59.412403Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"'934.3 KB'"},"metadata":{}}]},{"cell_type":"code","source":"def make_figure(*series, title=\"\", xtitle=\"\", ytitle=\"\"):\n    fig = go.Figure()\n    series=list(series)\n    x = series.pop(0)\n    for s in series:\n        fig.add_trace(go.Scatter(y=s, x=x))\n    fig.update_layout(\n        title= dict(text=title,\n                    x=0.5,\n                    xanchor='center'),\n        xaxis=dict(\n            title=xtitle,\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        yaxis=dict(\n            title=ytitle,\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        showlegend=False\n    )\n\n    return fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.438678Z","iopub.execute_input":"2023-11-20T06:22:59.439110Z","iopub.status.idle":"2023-11-20T06:22:59.447494Z","shell.execute_reply.started":"2023-11-20T06:22:59.439077Z","shell.execute_reply":"2023-11-20T06:22:59.446349Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"stock_series = []\nprob_memory = []\n\namplifier=1000\ndate=478\nstock=100\n#be cautious when setting dt. dt is inverse proportional to learning time dt=1 means no optimization\ndt=1\n\n#filename=\"IndexWap_day231.csv\"\n#excelfilereader=pd.read_csv(r'/content/kaggle_optiver_data/IndexWap_day231.csv', header=None)\n#data = excelfilereader.iloc[0,:].values.tolist()\n#data = excelfilereader.iloc[0,:]*1000\n\n#excelfilereader=pd.read_csv(r'/kaggle/input/optiver-trading-at-the-close/train.csv')\nexcelfilereader=pd.read_csv(r'/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv')\ndata_sub=excelfilereader[excelfilereader['stock_id']==stock]\ndata_sub2=data_sub[data_sub['date_id']==date]\ndata=pd.DataFrame(data_sub2,columns=['wap'])\ndata = data.values.tolist()\ndata=np.squeeze(data)\ndata=data*amplifier\n\n\nstock_series=data\ntime_series=range(0,data.shape[0])\ndf = pd.DataFrame({'dateindex': time_series, 'stock': stock_series})\n\n\n\nmake_figure(df['dateindex'], df['stock'],\n            title=\"Stock {} price at date {}\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\nx=df['dateindex']\ny=df['stock']\nx_sub = np.linspace(min(x), max(x), num=int(df.shape[0]/dt))\ny_sub = pchip_interpolate(x, y, x_sub)\ndf_new = pd.DataFrame({'dateindex': x_sub, 'stock': y_sub})\n\nmake_figure(df_new['dateindex'], df_new['stock'],\n            title=\"Stock {} price at date {} after interpolation\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.481248Z","iopub.execute_input":"2023-11-20T06:22:59.481940Z","iopub.status.idle":"2023-11-20T06:22:59.646560Z","shell.execute_reply.started":"2023-11-20T06:22:59.481903Z","shell.execute_reply":"2023-11-20T06:22:59.645755Z"},"trusted":true},"execution_count":87,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"3813642f-16da-42b6-bcb7-ee6383ac468f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3813642f-16da-42b6-bcb7-ee6383ac468f\")) {                    Plotly.newPlot(                        \"3813642f-16da-42b6-bcb7-ee6383ac468f\",                        [{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54],\"y\":[1000.0,997.378,997.955,997.869,997.284,997.46,997.4300000000001,997.313,997.2090000000001,997.169,997.2339999999999,997.213,997.017,996.888,997.235,997.0830000000001,996.9259999999999,996.9259999999999,996.971,997.3109999999999,997.5039999999999,996.793,997.325,997.298,997.155,996.6239999999999,996.633,996.402,996.051,996.75,997.3149999999999,995.641,995.657,995.651,995.696,995.6429999999999,997.8470000000001,997.934,997.8470000000001,997.738,997.76,997.808,997.8209999999999,997.893,997.636,997.7139999999999,997.7629999999999,997.659,997.765,997.789,997.848,997.698,997.598,995.835,995.873],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('3813642f-16da-42b6-bcb7-ee6383ac468f');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"39109a16-2e20-40de-8956-77ad0b7a4435\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"39109a16-2e20-40de-8956-77ad0b7a4435\")) {                    Plotly.newPlot(                        \"39109a16-2e20-40de-8956-77ad0b7a4435\",                        [{\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0],\"y\":[1000.0,997.378,997.955,997.869,997.284,997.46,997.4300000000001,997.313,997.2090000000001,997.169,997.2339999999999,997.213,997.017,996.888,997.235,997.0830000000001,996.9259999999999,996.9259999999999,996.971,997.3109999999999,997.5039999999999,996.793,997.325,997.298,997.155,996.6239999999999,996.633,996.402,996.051,996.75,997.3149999999999,995.641,995.657,995.651,995.696,995.6429999999999,997.8470000000001,997.934,997.8470000000001,997.738,997.76,997.808,997.8209999999999,997.893,997.636,997.7139999999999,997.7629999999999,997.659,997.765,997.789,997.848,997.698,997.598,995.835,995.873],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478 after interpolation\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('39109a16-2e20-40de-8956-77ad0b7a4435');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"index_1=[0,6,12,18,24,30,36,42,48,54]\nindex_2=[1,7,13,19,25,31,37,43,49]\nindex_3=[2,8,14,20,26,32,38,44,50]\nindex_4=[3,9,15,21,27,33,39,45,51]\nindex_5=[4,10,16,22,28,34,40,46,52]\nindex_6=[5,11,17,23,29,35,41,47,53]\nindex_list=[index_1,index_2,index_3,index_4,index_5, index_6]\n#index_list\nmake_figure(df_new['dateindex'][index_6], df_new['stock'][index_6],\n            title=\"Stock {} price at date {} sub 1\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.648424Z","iopub.execute_input":"2023-11-20T06:22:59.648775Z","iopub.status.idle":"2023-11-20T06:22:59.673392Z","shell.execute_reply.started":"2023-11-20T06:22:59.648741Z","shell.execute_reply":"2023-11-20T06:22:59.672209Z"},"trusted":true},"execution_count":88,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"f8396280-5711-4434-8bc4-443324099b24\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f8396280-5711-4434-8bc4-443324099b24\")) {                    Plotly.newPlot(                        \"f8396280-5711-4434-8bc4-443324099b24\",                        [{\"x\":[5.0,11.0,17.0,23.0,29.0,35.0,41.0,47.0,53.0],\"y\":[997.46,997.213,996.9259999999999,997.298,996.75,995.6429999999999,997.808,997.659,995.835],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478 sub 1\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('f8396280-5711-4434-8bc4-443324099b24');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"def indexWAP(data, date, t):\n    coefficient = [0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.002,\n                   0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004, 0.002, 0.002, 0.004, 0.002,\n                   0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004, 0.004, 0.004, 0.006, 0.002, 0.002, 0.04,\n                   0.002, 0.002, 0.004, 0.04, 0.002, 0.001, 0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002,\n                   0.006, 0.004, 0.006, 0.004, 0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004,\n                   0.002, 0.004, 0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n                   0.02, 0.004, 0.006, 0.002, 0.02, 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02, 0.006, 0.001, 0.002,\n                   0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002, 0.004, 0.006, 0.006, 0.001, 0.04,\n                   0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002, 0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008,\n                   0.006, 0.004, 0.002, 0.006, 0.002, 0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008,\n                   0.006, 0.008, 0.002, 0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001,\n                   0.002, 0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002, 0.04,\n                   0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02, 0.004, 0.002, 0.006, 0.02, 0.001, 0.002, 0.006,\n                   0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04, 0.002, 0.008, 0.002, 0.004, 0.001,\n                   0.004, 0.006, 0.004]\n    date_sub = data.loc[(data[\"date_id\"] == date) & (data[\"seconds_in_bucket\"] == t)]\n    stk_id = pd.DataFrame(date_sub, columns=[\"stock_id\"])\n    wap = pd.DataFrame(date_sub, columns=[\"wap\"])\n    stk_id_arr = np.array(stk_id).reshape(-1)\n\n    missing_elemnts = [item for item in range(stk_id_arr[0], 200) if item not in stk_id_arr]\n    missing_elemnts_arr = np.array(missing_elemnts).reshape(-1)\n    list_coefficient = coefficient\n    for i in sorted(missing_elemnts_arr, reverse=True):\n        list_coefficient.pop(i)\n\n    norm = [float(i) / sum(list_coefficient) for i in list_coefficient]\n    stock_index = np.dot(norm, wap)\n    return (np.array([stock_index]).item())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.675486Z","iopub.execute_input":"2023-11-20T06:22:59.676101Z","iopub.status.idle":"2023-11-20T06:22:59.697231Z","shell.execute_reply.started":"2023-11-20T06:22:59.676052Z","shell.execute_reply":"2023-11-20T06:22:59.696133Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"time_arr = np.arange(0, 550, 10)\ndate_0 = excelfilereader.loc[(excelfilereader[\"date_id\"] == date)]\n\nindex = []\n\nfor i in time_arr:\n        index.append(indexWAP(date_0, date, i))\n\nindex=np.squeeze(index)\nindex=index*amplifier\ndf_index = pd.DataFrame({'dateindex': x_sub, 'stock': index})\n\nmake_figure(df_index['dateindex'], df_index['stock'],\n            title=\"Index at date {}\".format(int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.700787Z","iopub.execute_input":"2023-11-20T06:22:59.701373Z","iopub.status.idle":"2023-11-20T06:22:59.911467Z","shell.execute_reply.started":"2023-11-20T06:22:59.701339Z","shell.execute_reply":"2023-11-20T06:22:59.910309Z"},"trusted":true},"execution_count":90,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"9161f31f-38c2-4846-98b7-026d355b9b5b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9161f31f-38c2-4846-98b7-026d355b9b5b\")) {                    Plotly.newPlot(                        \"9161f31f-38c2-4846-98b7-026d355b9b5b\",                        [{\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0],\"y\":[999.9999999999994,1000.1934979999995,1000.2803799999991,1000.5057419999991,1000.5776239999993,1000.9211489999997,1001.0868889999991,1001.4386759999993,1001.6055709999994,1001.571023999999,1001.2883839999993,1001.2153459999995,1001.3753339999996,1001.4475599999993,1001.4412769999992,1001.2090269999993,1000.7005779999992,1000.6252939999996,1000.7831449999991,1000.9719369999992,1000.9638659999994,1001.1076719999994,1001.1015999999993,1001.1915879999993,1001.1325089999993,1001.0909859999992,1000.9588159999994,1000.9791729999993,1000.9943929999994,1001.0193049999991,1000.4120459999995,1000.2582529999993,1000.1924049999996,1000.2305739999993,1000.5569859999993,1000.4650349999993,1000.4069959999995,1000.6807949999994,1000.6606849999993,1000.7183519999994,1000.6872139999994,1000.7778859999994,1000.8868109999993,1001.0838919999993,1001.1868369999992,1001.1565879999993,1001.0677419999996,1001.1097629999992,1001.1437979999993,1001.2114599999993,1001.2380639999993,1001.2808689999995,1001.3632789999996,1001.3999409999994,1001.4106029999994],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Index at date 478\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('9161f31f-38c2-4846-98b7-026d355b9b5b');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"def user_gen(df,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    while t<=(df.shape[0]):\n        stock_price= df['stock'][t]\n        yield(t, stock_price, trend_index)\n        t+=1\n\ndef user_gen_dt(df,dt=1,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    x=df['dateindex']\n    y=df['stock']\n    x_sub = np.linspace(min(x), max(x), num=int(df.shape[0]/dt))\n  #print(np.shape(x_sub))\n    y_sub = pchip_interpolate(x, y, x_sub)\n  #print(np.shape(y_sub))\n    while t<=int(len(x_sub)/dt):\n        stock_price= y_sub[t]\n        yield(t, stock_price, trend_index)\n        t+=dt\n\ndef user_sub(df,index_sub,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    x=range(len(index_sub))\n    y=[]\n    #x=df['dateindex'][index_sub]\n    y=df['stock'][index_sub]\n    for i in range(len(index_sub)):\n        y[i]=df['stock'][index_sub[i]]\n    while t<=int(len(x)):\n        stock_price= y[t]\n        yield(t, stock_price, trend_index)\n        t+=1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.913397Z","iopub.execute_input":"2023-11-20T06:22:59.913849Z","iopub.status.idle":"2023-11-20T06:22:59.925093Z","shell.execute_reply.started":"2023-11-20T06:22:59.913798Z","shell.execute_reply":"2023-11-20T06:22:59.924150Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"def market_gen(gen, lag=2):\n\n    buffer = []\n    diffbuffer = []\n\n\n    # fill buffer\n    dt, last, trend = next(gen)\n    for i in range(lag):\n        prev = last\n        dt, last, trend = next(gen)\n        buffer.append(last-trend)\n        diffbuffer.append(last-prev)\n\n    # yield first group of lag vals and diffs\n    yield buffer+diffbuffer\n\n    while(True):\n        prev = last\n        dt, last, trend = next(gen)\n        buffer.pop(0)\n        buffer.append(last-trend)\n        diffbuffer.pop(0)\n        diffbuffer.append(last-prev)\n        yield buffer+diffbuffer\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.926291Z","iopub.execute_input":"2023-11-20T06:22:59.927265Z","iopub.status.idle":"2023-11-20T06:22:59.941378Z","shell.execute_reply.started":"2023-11-20T06:22:59.927233Z","shell.execute_reply":"2023-11-20T06:22:59.940324Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"class Market:\n    \"\"\"Follows OpenAI gym environment convention basically\n    init with generator and number of stocks\n    reset() - generate and return first state\n    step() - generate next state and reward\n    \"\"\"\n    def __init__(self, gen, lag=16, nstocks=1, episode_length=300):\n        self.genfunc = gen\n        self.nstocks = nstocks\n        self.episode_length = episode_length\n        self.t = 0\n        self.total_reward = 0\n        self.lag = lag\n        self.observation_space = np.asarray([1] * nstocks * lag * 2,)\n        self.state_size = nstocks * lag * 2\n        self.action_size = 2\n\n    def reset(self):\n        self.t = 0\n        self.total_reward = 0\n        self.gen = [self.genfunc() for _ in range(self.nstocks)]\n        self.state=[next(g) for g in self.gen]\n        self.state = np.asarray([s for s in self.state])\n        return self.state\n\n    def render(self):\n        print(self.state[0, nstocks-1])\n\n    def step(self, action):\n        action = np.asarray([action])\n        try:\n            self.state=[next(g) for g in self.gen]\n        except StopIteration:\n            return print(\"generator failed.\\n\")\n        \n        self.state = np.asarray([s for s in self.state])\n        # last element is most recent change\n        stock_delta = np.asarray([s[-1] for s in self.state])\n        # element at lag-1 is most recent deviation\n        market_price = np.asarray([s[self.lag-1]+100 for s in self.state])\n        # map actions 0 1 2 to positions -1, 0, 1\n        position = action - 1\n        reward = position @ stock_delta\n        self.total_reward += reward\n        self.t += 1\n        done = True if self.episode_length and self.t >= self.episode_length else False\n        # state, reward, done, info\n        return self.state, reward, done, market_price\n\n    def close(self):\n        pass\n\n\n#env = Market(user_gen_dt(df,0.1), lag=1, nstocks=1, episode_length=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.944081Z","iopub.execute_input":"2023-11-20T06:22:59.944955Z","iopub.status.idle":"2023-11-20T06:22:59.961079Z","shell.execute_reply.started":"2023-11-20T06:22:59.944905Z","shell.execute_reply":"2023-11-20T06:22:59.959994Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"DISCOUNT_RATE = 0\n# WIN_REWARD = 10\nEPSILON_DECAY = 0.995\nSAMPLE_SIZE = 256\nRENDER = False\nOUTPUT_DIR = 'model_output/trading/'\n\nclass DQN_Agent:\n    def __init__(self, state_size, action_size, filename=\"dqn\",\n                 discount_rate=DISCOUNT_RATE,\n                 learning_rate=0.001,\n                 epsilon=1.0,\n                 epsilon_decay=EPSILON_DECAY,\n                 epsilon_min=0.01):\n\n        self.state_size = state_size\n        self.action_size = action_size\n        self.filename = filename\n        self.discount_rate = discount_rate\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.learning_rate = learning_rate\n\n        self.model = self.build_model()\n        self.memory = pd.DataFrame(columns=[\"state\", \"action\", \"next_state\",\n                                            \"reward\", \"done\"])\n        self.memory_size = 100000\n        self.results = []\n        self.train_batch_size = 1\n        self.timestep = 0\n        self.save_interval = 10\n\n    def build_model(self,\n                    n_hidden_layers=2,\n                    hidden_layer_size=16,\n                    activation='relu',\n                    reg_penalty=0.001,\n                    dropout=0.0675,\n                    verbose=True\n                   ):\n        \"\"\"return keras NN model per inputs\n        input is a state - array of size state_size\n        output is an array of action values - array of size action_size\n        \"\"\"\n\n        inputs = Input(shape=(self.state_size,), name=\"Input\")\n        last_layer = inputs\n\n        for i in range(n_hidden_layers):\n            if verbose:\n                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n                print(formatstr % (i + 1,\n                                   hidden_layer_size,\n                                   activation,\n                                   reg_penalty,\n                                   dropout))\n            # add dropout, but not on inputs, only between hidden layers\n            if i and dropout:\n                last_layer = Dropout(dropout, name=\"Dropout%02d\" % i)(last_layer)\n\n            last_layer = Dense(units=hidden_layer_size,\n                               activation=activation,\n                               kernel_initializer=glorot_uniform(),\n                               kernel_regularizer=l2(reg_penalty),\n                               name=\"Dense%02d\" % i)(last_layer)\n\n        outputs = Dense(self.action_size, activation='linear', name=\"Output\")(last_layer)\n\n        #model = Model(inputs=input_layer , output=last_layer)\n        model = Model(inputs=inputs, outputs=outputs)\n\n        if verbose:\n            print(model.summary())\n\n        model.compile(loss='mse', optimizer=Adam(\n            #learning_rate=self.learning_rate\n        ))\n\n        return model\n\n    def remember(self):\n        \"\"\"store the states and rewards needed to fit the model\"\"\"\n        # append in place\n        self.memory.loc[self.memory.shape[0]] = [self.state,\n                                                 self.action,\n                                                 self.next_state,\n                                                 self.reward,\n                                                 self.done]\n\n    def train(self):\n        \"\"\"train the model on experience stored by remember\"\"\"\n\n        # need at least SAMPLE_SIZE observations\n        if self.memory.shape[0] < SAMPLE_SIZE:\n            return\n\n        # truncate memory\n        self.memory = self.memory[-self.memory_size:]\n        # sample sample_size observations from memory\n        minibatch = self.memory.sample(n=SAMPLE_SIZE)\n\n        # target is our best estimate of value of each action\n        X_fit = np.concatenate(minibatch['state'].values)\n        X_fit = X_fit.reshape((SAMPLE_SIZE, self.state_size))\n        Y_pred = self.model.predict(X_fit)\n\n        # we don't just fit model against model's own prediction, gets us nowhere\n        # we improve the target by what we learned about the action we actually took\n        # value is reward obtained + predicted value of the observed next state\n        minibatch['target_observed'] = minibatch['reward']\n        # if done, target is the reward\n        # reward by gym env is only 1 for each timestep of survival\n        # but we also added a reward of -10 on failure\n        # if not done, add discount_rate  * Q-value prediction for  observed next state\n        not_done = minibatch.loc[minibatch['done'] == False]\n        X_observed = np.concatenate(not_done['next_state'].values)\n        X_observed = X_observed.reshape((not_done.shape[0], self.state_size))\n        # run all predictions at once\n        # iterates faster but does not train after each prediction\n        y_observed_pred = np.amax(self.model.predict(X_observed), axis=1)\n        minibatch.loc[minibatch['done'] == False, 'target_observed'] \\\n            += self.discount_rate * y_observed_pred\n        # vectorized vlookup - update col specified by action with target_observed\n        np.put_along_axis(Y_pred,\n                          minibatch['action'].astype(int).values.reshape(SAMPLE_SIZE, 1),\n                          minibatch['target_observed'].values.reshape(SAMPLE_SIZE, 1),\n                          axis=1)\n        # fit model against improved target\n        # arbitrary 8 batch size to reduce variance a little and speed up fit\n        self.model.fit(X_fit, Y_pred,\n                       epochs=1,\n                       batch_size=self.train_batch_size,\n                       verbose=0)\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def act(self, state):\n        \"\"\"pick an action using model\"\"\"\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])\n\n    def save(self):\n        \"save agent: pickle self and use Keras native save model\"\n        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n        self.model.save(\"%s.h5\" % fullname)\n        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n\n    def load(filename):\n        \"load saved agent\"\n        new = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n        new.model = load_model(\"%s.h5\" % filename)\n        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(new.results),\n                                                                      len(new.memory),\n                                                                      new.epsilon))\n        return new\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        self.total_reward = 0\n\n    def increment_time(self):\n        \"\"\"increment timestep counter\"\"\"\n        self.timestep += 1\n\n    def save_score(self):\n        \"\"\"save score of each episode\"\"\"\n        self.results.append(self.total_reward)\n\n    def score_episode(self, episode_num, n_episodes):\n        \"\"\"output results and save\"\"\"\n        self.save_score()\n        avglen = min(len(self.results), self.save_interval)\n        formatstr = \"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n        print(formatstr.format(time.strftime(\"%H:%M:%S\"), len(self.results),\n                               n_episodes, self.total_reward, avglen,\n                               sum(self.results[-avglen:])/avglen, memusage()),\n              end=\"\\r\", flush=False)\n\n    def run_episode(self, render=RENDER):\n        \"\"\"run a full episode\"\"\"\n        global env\n\n        self.reset()\n        self.state = env.reset()\n        self.done = False\n\n        while not self.done:\n            if render:\n                env.render()\n            self.action = self.act(self.state.reshape([1, self.state_size]))\n            self.next_state, self.reward, self.done, _ = env.step(self.action)\n            self.total_reward += self.reward\n            self.remember()\n            self.state = self.next_state\n            self.increment_time()\n\n        if render:\n            env.render()\n\n        self.train()\n\n    def rlplot(self, title='Agent Training Progress'):\n        \"\"\"plot training progress\"\"\"\n        df = pd.DataFrame({'timesteps': self.results})\n        df['avg'] = df['timesteps'].rolling(10).mean()\n\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['timesteps'],\n                                 mode='markers',\n                                 name='timesteps',\n                                 marker=dict(\n                                     color='mediumblue',\n                                     size=4,\n                                 ),\n                                ))\n\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['avg'],\n                                 mode='lines',\n                                 line_width=3,\n                                 name='moving average'))\n\n        fig.update_layout(\n            title=dict(text=title,\n                       x=0.5,\n                       xanchor='center'),\n            xaxis=dict(\n                title=\"Episodes\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            yaxis=dict(\n                title=\"Total Reward per Episode\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            legend=go.layout.Legend(\n                x=0.01,\n                y=0.99,\n                traceorder=\"normal\",\n                font=dict(\n                    family=\"sans-serif\",\n                    size=12,\n                    color=\"black\"\n                ),\n                #bgcolor=\"LightSteelBlue\",\n                bordercolor=\"Black\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:22:59.963109Z","iopub.execute_input":"2023-11-20T06:22:59.963581Z","iopub.status.idle":"2023-11-20T06:23:00.012432Z","shell.execute_reply.started":"2023-11-20T06:22:59.963483Z","shell.execute_reply":"2023-11-20T06:23:00.011435Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"RENDER = False\nOUTPUT_DIR = 'model_output/trading/'\n\nclass Agent:\n    \"\"\"abstract base class for agents\"\"\"\n\n    def __init__(self, state_size, action_size, filename=\"model\",\n                 *args, **kwargs):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.filename = filename\n        self.timestep = 0\n        self.total_reward = 0\n        self.save_interval = 10\n\n        raise NotImplementedError\n\n    def build_model(self, *args, **kwargs):\n        \"\"\"build the relevant model\"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        self.total_reward = 0\n\n    def increment_time(self):\n        \"\"\"increment timestep counter\"\"\"\n        self.timestep += 1\n\n    def remember(self, *args, **kwargs):\n        \"\"\"store the states and rewards needed to fit the model\"\"\"\n        raise NotImplementedError\n\n    def train(self, *args, **kwargs):\n        \"\"\"train the model on experience stored by remember\"\"\"\n        raise NotImplementedError\n\n    def act(self, *args, **kwargs):\n        \"\"\"pick an action using model\"\"\"\n        raise NotImplementedError\n\n    def save_score(self):\n        \"\"\"save score of each episode\"\"\"\n        self.results.append(self.total_reward)\n\n    def score_episode(self, episode_num, n_episodes):\n        \"\"\"output results and save\"\"\"\n        self.save_score()\n        avglen = min(len(self.results), self.save_interval)\n        formatstr = \"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n        print(formatstr.format(time.strftime(\"%H:%M:%S\"), len(self.results),\n                               n_episodes, self.total_reward, avglen,\n                               sum(self.results[-avglen:])/avglen, memusage()),\n              end=\"\\r\", flush=False)\n\n    def run_episode(self, render=RENDER):\n        \"\"\"run a full episode\"\"\"\n        global env\n\n        self.reset()\n        self.state = env.reset()\n        self.done = False\n\n        while not self.done:\n            if render:\n                env.render()\n            self.action = self.act(self.state.reshape([1, self.state_size]))\n            self.next_state, self.reward, self.done, _ = env.step(self.action)\n            self.total_reward += self.reward\n\n            self.remember()\n            self.state = self.next_state\n            self.increment_time()\n\n        if render:\n            env.render()\n\n        self.train()\n\n    def save(self, *args, **kwargs):\n        \"\"\"save agent to disk\"\"\"\n        raise NotImplementedError\n\n    def load(*args, **kwargs):\n        \"\"\"load agent from disk\"\"\"\n        raise NotImplementedError\n\n    def view(self):\n        \"\"\"Run an episode without training, with rendering\"\"\"\n        state = env.reset()\n        state = np.reshape(state, [1, self.state_size])\n        done = False\n\n        # run an episode\n        self.timestep = 0\n        r = 0\n        while not done:\n            env.render()\n            action = self.act(state)\n            lastmarket = self.state[0, nstocks-1]\n            state, reward, done, _ = env.step(action)\n            newmarket = self.state[0, nstocks-1]\n            print(\"prev mkt: %.4f action: %d, new mkt %f, reward %f\" % (lastmarket, action, newmarket, reward))\n            r += reward\n            state = np.reshape(state, [1, self.state_size])\n            self.timestep += 1\n        env.render()\n        print(r)\n        env.close()\n        return self.timestep\n\n    def rlplot(self, title='Trading Agent Training Progress'):\n        \"\"\"plot training progress\"\"\"\n        df = pd.DataFrame({'timesteps': self.results})\n        df['avg'] = df['timesteps'].rolling(10).mean()\n\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['timesteps'],\n                                 mode='markers',\n                                 name='timesteps',\n                                 marker=dict(\n                                     color='mediumblue',\n                                     size=4,\n                                 ),\n                                ))\n\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['avg'],\n                                 mode='lines',\n                                 line_width=3,\n                                 name='moving average'))\n\n        fig.update_layout(\n            title=dict(text=title,\n                       x=0.5,\n                       xanchor='center'),\n            xaxis=dict(\n                title=\"Episodes\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            yaxis=dict(\n                title=\"Total Reward per Episode\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            legend=go.layout.Legend(\n                x=0.01,\n                y=0.99,\n                traceorder=\"normal\",\n                font=dict(\n                    family=\"sans-serif\",\n                    size=12,\n                    color=\"black\"\n                ),\n                #bgcolor=\"LightSteelBlue\",\n                bordercolor=\"Black\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:23:00.014221Z","iopub.execute_input":"2023-11-20T06:23:00.014784Z","iopub.status.idle":"2023-11-20T06:23:00.045186Z","shell.execute_reply.started":"2023-11-20T06:23:00.014740Z","shell.execute_reply":"2023-11-20T06:23:00.044146Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"class REINFORCE_Agent(Agent):\n    \"\"\"REINFORCE policy gradient method using deep Keras NN\"\"\"\n    def __init__(self, state_size=4, action_size=2, learning_rate=0.0005,\n                 discount_rate=DISCOUNT_RATE, n_hidden_layers=2, hidden_layer_size=16,\n                 activation='relu', reg_penalty=0, dropout=0, filename=\"kreinforce\",\n                 verbose=True):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.action_space = list(range(action_size))\n        self.learning_rate = learning_rate\n        self.discount_rate = discount_rate\n\n        self.n_hidden_layers = n_hidden_layers\n        self.hidden_layer_size = hidden_layer_size\n        self.activation = activation\n        self.reg_penalty = reg_penalty\n        self.dropout = dropout\n        self.verbose = verbose\n        self.filename = filename\n\n        self.train_model, self.predict_model = self.policy_model()\n        self.results = []\n        self.save_interval = 10\n        self.reset()\n\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        # truncate memory\n        self.state_memory = []\n        self.action_memory = []\n        self.reward_memory = []\n        self.total_reward = 0\n\n    def policy_model(self):\n        \"\"\"set up NN model for policy.\n        predict returns probs of actions to sample from.\n        train needs discounted rewards for the episode, so we define custom loss.\n        when training use train_model with custom loss and multi input of training data and rewards.\n        when predicting use predict_model with single input.\n        \"\"\"\n\n        def custom_loss(y_true, y_pred):\n            y_pred_clip = K.clip(y_pred, 1e-8, 1-1e-8)\n            log_likelihood = y_true*K.log(y_pred_clip)\n            return K.sum(-log_likelihood*discounted_rewards)\n\n        inputs = Input(shape=(self.state_size,), name=\"Input\")\n        discounted_rewards = Input(shape=(1,), name=\"Discounted_rewards\")\n        last_layer = inputs\n\n        for i in range(self.n_hidden_layers):\n            if self.verbose:\n                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n                print(formatstr % (i + 1,\n                                   self.hidden_layer_size,\n                                   self.activation,\n                                   self.reg_penalty,\n                                   self.dropout,\n                                   ))\n            # add dropout, but not on inputs, only between hidden layers\n            if i and self.dropout:\n                last_layer = Dropout(self.dropout, name=\"Dropout%02d\" % i)(last_layer)\n\n            last_layer = Dense(units=self.hidden_layer_size,\n                               activation=self.activation,\n                               kernel_initializer=glorot_uniform(),\n                               kernel_regularizer=keras.regularizers.l2(self.reg_penalty),\n                               name=\"Dense%02d\" % i)(last_layer)\n\n        outputs = Dense(self.action_size, activation='softmax', name=\"Output\")(last_layer)\n\n        train_model = Model(inputs=[inputs, discounted_rewards], outputs=[outputs])\n        train_model.compile(optimizer=Adam(lr=self.learning_rate), loss=custom_loss)\n\n        predict_model = Model(inputs=[inputs], outputs=[outputs])\n\n        if self.verbose:\n            print(predict_model.summary())\n\n        return train_model, predict_model\n\n\n    def act(self, state):\n        \"\"\"pick an action using predict_model\"\"\"\n        probabilities = self.predict_model.predict(state)\n\n        action = np.random.choice(self.action_space, p=probabilities[0])\n        prob=probabilities[0][action]\n        #print(\"probability={}\\n\".format(probabilities[0][action]))\n        prob_memory.append(prob)\n        return action\n\n    def remember(self):\n        \"\"\"at each step save state, action, reward for future training\"\"\"\n\n        self.state_memory.append(self.state)\n        self.action_memory.append(self.action)\n        self.reward_memory.append(self.reward)\n        \n\n    def train(self):\n        \"\"\"train the model on experience stored by remember\"\"\"\n        state_memory = np.array(self.state_memory)\n        state_memory = state_memory.reshape((len(self.state_memory),self.state_size))\n        action_memory = np.array(self.action_memory)\n        reward_memory = np.array(self.reward_memory)\n\n        # one-hot actions\n        actions = np.zeros([len(action_memory), self.action_size])\n        actions[np.arange(len(action_memory)), action_memory] = 1\n\n        disc_rewards = np.zeros_like(reward_memory)\n        cumulative_rewards = 0\n        for i in reversed(range(len(reward_memory))):\n            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n            disc_rewards[i] = cumulative_rewards\n\n        # standardize\n        disc_rewards -= np.mean(disc_rewards)\n        disc_rewards /= np.std(disc_rewards) if np.std(disc_rewards) > 0 else 1\n\n        # train states v. actions, (complemented by disc_rewards_std)\n        cost = self.train_model.train_on_batch([state_memory, disc_rewards], actions)\n\n        return cost\n\n    def view(self):\n        \"\"\"Run an episode without training, with rendering\"\"\"\n        state = env.reset()\n        state = np.reshape(state, [1, self.state_size])\n        done = False\n\n        # run an episode\n        self.timestep = 0\n        r = 0\n        retarray = []\n        while not done:\n            action = self.act(state)\n            lastmarket = state[0, self.state_size//2-1]\n            state, reward, done, _ = env.step(action)\n            newmarket = state[0, self.state_size//2-1]\n            print(\"prev mkt: %.4f action: %d, new mkt %.4f, reward %f\" % (lastmarket, action, newmarket, reward))\n            r += reward\n            state = np.reshape(state, [1, self.state_size])\n            self.timestep += 1\n            retarray.append((self.timestep, action, lastmarket, newmarket, reward))\n        print(r)\n        env.close()\n        return retarray\n\n    def save(self):\n        \"save agent: pickle self and use Keras native save model\"\n        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n        self.predict_model.save(\"%s_predict.h5\" % fullname)\n        # can't save / load train model due to custom loss\n        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n\n    def load(filename, memory=True):\n        \"load saved agent\"\n        self = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n        self.predict_model = load_model(\"%s_predict.h5\" % filename)\n        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(self.results),\n                                                                      len(self.memory),\n                                                                      self.epsilon))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:23:00.047080Z","iopub.execute_input":"2023-11-20T06:23:00.047567Z","iopub.status.idle":"2023-11-20T06:23:00.085018Z","shell.execute_reply.started":"2023-11-20T06:23:00.047526Z","shell.execute_reply":"2023-11-20T06:23:00.083942Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"#dt=1\n#index_1=[0,6,12,18,24,30,36,42,48,54]\n#index_2=[1,7,13,19,25,31,37,43,49]\n#index_3=[2,8,14,20,26,32,38,44,50]\n#index_4=[3,9,15,21,27,33,39,45,51]\n#index_5=[4,10,16,22,28,34,40,46,52]\n#index_6=[5,11,17,23,29,35,41,47,53]\n\n#index_pick=index_5\nN_EPISODES = 1500\n#lag is fixed\nlag = 1\n#ticks_per_episode = len(index_pick)/dt-1-lag\nnstocks = 1\n\n\n#gen = user_sub(df_new,index_1,start_trend=amplifier)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:23:00.086723Z","iopub.execute_input":"2023-11-20T06:23:00.087161Z","iopub.status.idle":"2023-11-20T06:23:00.099680Z","shell.execute_reply.started":"2023-11-20T06:23:00.087122Z","shell.execute_reply":"2023-11-20T06:23:00.098734Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"def shm_market_gen():\n    return market_gen(gen=user_sub(df_new,index_pick,start_trend=amplifier),\n                      lag=lag)\n\n\ngen=shm_market_gen()\ntime_series=[]\nstock_series=[]\nfor i in range(len(index_pick)-lag):\n    z = next(gen)\n    time_series.append(i)\n    stock_series.append(z[1])\n\ndf_gen = pd.DataFrame({'dateindex': time_series, 'stock': stock_series})\n\nmake_figure(df_gen['dateindex'], df_gen['stock'],\n            title='Genarated market',\n            xtitle='Timesteps',\n            ytitle='buffer+diffbuffer'\n           )","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:23:00.101031Z","iopub.execute_input":"2023-11-20T06:23:00.101427Z","iopub.status.idle":"2023-11-20T06:23:00.133291Z","shell.execute_reply.started":"2023-11-20T06:23:00.101398Z","shell.execute_reply":"2023-11-20T06:23:00.132231Z"},"trusted":true},"execution_count":98,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"54441bdc-20bb-46ee-aed5-ad768f03969f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"54441bdc-20bb-46ee-aed5-ad768f03969f\")) {                    Plotly.newPlot(                        \"54441bdc-20bb-46ee-aed5-ad768f03969f\",                        [{\"x\":[0,1,2,3,4,5,6,7],\"y\":[-0.05000000000006821,-0.3079999999999927,0.3990000000001146,-1.274000000000001,-0.3550000000000182,2.0639999999999645,0.0029999999999290594,-0.16499999999996362],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Genarated market\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"buffer+diffbuffer\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('54441bdc-20bb-46ee-aed5-ad768f03969f');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"result_index=[]\n\nfor i in range(len(index_list)):\n    ticks_per_episode = len(index_list[i])/dt-1-lag\n    def shm_market_gen():\n        return market_gen(gen=user_sub(df_index,index_list[i],start_trend=amplifier),\n                      lag=1)\n\n    env = Market(shm_market_gen,\n             lag=lag,\n             nstocks=1,\n             episode_length=ticks_per_episode)\n\n    agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n                        action_size=3,\n                       )\n\n    start_time = time.time()\n    print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n    for e in range(N_EPISODES):\n        agent.run_episode()\n        agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n    elapsed_time = time.time() - start_time\n    print(\"\\nTrain time: \", elapsed_time)\n    env.reset()\n    z = agent.view()\n\n    df = pd.DataFrame(z)\n    df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n    df['lastmarket']+=1\n    df['newmarket']+=1\n    df['short'] = np.nan\n    df.loc[df['action']==0, 'short'] = df['newmarket']\n    df['flat'] = np.nan\n    df.loc[df['action']==1, 'flat'] = df['newmarket']\n    df['long'] = np.nan\n    df.loc[df['action']==2, 'long'] = df['newmarket']\n    df['totalreward'] = df['reward'].cumsum()\n    df.to_csv('df_index_list_{}.csv'.format(i))\n    prob=agent.predict_model.predict(agent.state_memory)\n    action = np.random.choice(agent.action_space, p=prob[0])\n    arr=np.array(df_index['stock'][index_list[i]])\n    mean_diff=np.mean(np.absolute(np.diff(arr)))\n    action_arr=np.array(df['action']-1)\n    pick_index=np.array(df_index['stock'][index_list[i]])\n    wap_future_index=[]\n    #assume in the first time index of t+60 the same as t\n    wap_future_index.append(pick_index[0])\n    for j in range(len(action_arr)):\n        wap_future_index.append(action_arr[j]*mean_diff+pick_index[j+1])\n    wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n    seconds_in_bucket=[j * 10 for j in index_list[i]]\n    comb=list(zip(seconds_in_bucket,pick_index,wap_future_index))\n    result_index.append(comb)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:23:00.135170Z","iopub.execute_input":"2023-11-20T06:23:00.135492Z","iopub.status.idle":"2023-11-20T06:26:57.685631Z","shell.execute_reply.started":"2023-11-20T06:23:00.135463Z","shell.execute_reply":"2023-11-20T06:26:57.684355Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"layer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_101\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:23:00\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:23:41 episode 1500/1500:, score: 2.2502900000005184, 10-episode avg: 1.6 Memory: 943.7 KB            \nTrain time:  40.76256489753723\nprev mkt: 1.0869 action: 2, new mkt 1.3753, reward 0.288445\nprev mkt: 1.3753 action: 0, new mkt 0.7831, reward 0.592189\nprev mkt: 0.7831 action: 2, new mkt 1.1325, reward 0.349364\nprev mkt: 1.1325 action: 0, new mkt 0.4120, reward 0.720463\nprev mkt: 0.4120 action: 0, new mkt 0.4070, reward 0.005050\nprev mkt: 0.4070 action: 2, new mkt 0.8868, reward 0.479815\nprev mkt: 0.8868 action: 0, new mkt 1.1438, reward -0.256987\nprev mkt: 1.1438 action: 0, new mkt 1.4106, reward -0.266805\n1.91153400000087\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_103\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:23:41\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:24:20 episode 1500/1500:, score: 2.134360000000015, 10-episode avg: 2.1 Memory: 943.7 KB            \nTrain time:  39.11512589454651\nprev mkt: 1.4387 action: 2, new mkt 1.4476, reward 0.008884\nprev mkt: 1.4476 action: 0, new mkt 0.9719, reward 0.475623\nprev mkt: 0.9719 action: 2, new mkt 1.0910, reward 0.119049\nprev mkt: 1.0910 action: 0, new mkt 0.2583, reward 0.832733\nprev mkt: 0.2583 action: 2, new mkt 0.6808, reward 0.422542\nprev mkt: 0.6808 action: 2, new mkt 1.0839, reward 0.403097\nprev mkt: 1.0839 action: 0, new mkt 1.2115, reward -0.127568\n2.134360000000015\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_105\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:24:20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:24:58 episode 1500/1500:, score: 1.9326729999996815, 10-episode avg: 1.5 Memory: 944.2 KB           \nTrain time:  38.083062410354614\nprev mkt: 1.6056 action: 0, new mkt 1.4413, reward 0.164294\nprev mkt: 1.4413 action: 0, new mkt 0.9639, reward 0.477411\nprev mkt: 0.9639 action: 0, new mkt 0.9588, reward 0.005050\nprev mkt: 0.9588 action: 0, new mkt 0.1924, reward 0.766411\nprev mkt: 0.1924 action: 2, new mkt 0.6607, reward 0.468280\nprev mkt: 0.6607 action: 2, new mkt 1.1868, reward 0.526152\nprev mkt: 1.1868 action: 1, new mkt 1.2381, reward 0.000000\n2.407597999999439\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_107\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"None\nStart: 06:24:58\n06:25:39 episode 1500/1500:, score: 1.2657109999998966, 10-episode avg: 1.5 Memory: 956.8 KB            \nTrain time:  40.732030153274536\nprev mkt: 1.5710 action: 0, new mkt 1.2090, reward 0.361997\nprev mkt: 1.2090 action: 0, new mkt 1.1077, reward 0.101355\nprev mkt: 1.1077 action: 0, new mkt 0.9792, reward 0.128499\nprev mkt: 0.9792 action: 0, new mkt 0.2306, reward 0.748599\nprev mkt: 0.2306 action: 2, new mkt 0.7184, reward 0.487778\nprev mkt: 0.7184 action: 0, new mkt 1.1566, reward -0.438236\nprev mkt: 1.1566 action: 0, new mkt 1.2809, reward -0.124281\n1.2657109999998966\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_109\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:25:39\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:26:19 episode 1500/1500:, score: 0.930726000000277, 10-episode avg: 1.1 Memory: 962.1 KB             \nTrain time:  39.63123893737793\nprev mkt: 1.2884 action: 0, new mkt 0.7006, reward 0.587806\nprev mkt: 0.7006 action: 2, new mkt 1.1016, reward 0.401022\nprev mkt: 1.1016 action: 0, new mkt 0.9944, reward 0.107207\nprev mkt: 0.9944 action: 0, new mkt 0.5570, reward 0.437407\nprev mkt: 0.5570 action: 0, new mkt 0.6872, reward -0.130228\nprev mkt: 0.6872 action: 0, new mkt 1.0677, reward -0.380528\nprev mkt: 1.0677 action: 0, new mkt 1.3633, reward -0.295537\n0.7271489999999403\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_111\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:26:19\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:26:57 episode 1500/1500:, score: 1.573694999999816, 10-episode avg: 1.5 Memory: 967.5 KB           \nTrain time:  38.30169129371643\nprev mkt: 1.2153 action: 0, new mkt 0.6253, reward 0.590052\nprev mkt: 0.6253 action: 2, new mkt 1.1916, reward 0.566294\nprev mkt: 1.1916 action: 0, new mkt 1.0193, reward 0.172283\nprev mkt: 1.0193 action: 0, new mkt 0.4650, reward 0.554270\nprev mkt: 0.4650 action: 2, new mkt 0.7779, reward 0.312851\nprev mkt: 0.7779 action: 0, new mkt 1.1098, reward -0.331877\nprev mkt: 1.1098 action: 0, new mkt 1.3999, reward -0.290178\n1.573694999999816\n","output_type":"stream"}]},{"cell_type":"code","source":"from itertools import chain\n\nresult_indx_list = list(chain.from_iterable(result_index))\n#print(result_indx_list)\nsorted_indx_result = sorted(result_indx_list, key=lambda x: x[0])\n#print(sorted_indx_result)\nseconds_in_bucket, wap_index, wap_index_future = zip(*sorted_indx_result)\nprint(list(seconds_in_bucket))\nprint(list(wap_index))\nprint(list(wap_index_future))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:26:57.687846Z","iopub.execute_input":"2023-11-20T06:26:57.689303Z","iopub.status.idle":"2023-11-20T06:26:57.697228Z","shell.execute_reply.started":"2023-11-20T06:26:57.689255Z","shell.execute_reply":"2023-11-20T06:26:57.696036Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540]\n[999.9999999999994, 1000.1934979999995, 1000.2803799999991, 1000.5057419999991, 1000.5776239999993, 1000.9211489999997, 1001.0868889999991, 1001.4386759999993, 1001.6055709999994, 1001.571023999999, 1001.2883839999993, 1001.2153459999995, 1001.3753339999996, 1001.4475599999993, 1001.4412769999992, 1001.2090269999993, 1000.7005779999992, 1000.6252939999996, 1000.7831449999991, 1000.9719369999992, 1000.9638659999994, 1001.1076719999994, 1001.1015999999993, 1001.1915879999993, 1001.1325089999993, 1001.0909859999992, 1000.9588159999994, 1000.9791729999993, 1000.9943929999994, 1001.0193049999991, 1000.4120459999995, 1000.2582529999993, 1000.1924049999996, 1000.2305739999993, 1000.5569859999993, 1000.4650349999993, 1000.4069959999995, 1000.6807949999994, 1000.6606849999993, 1000.7183519999994, 1000.6872139999994, 1000.7778859999994, 1000.8868109999993, 1001.0838919999993, 1001.1868369999992, 1001.1565879999993, 1001.0677419999996, 1001.1097629999992, 1001.1437979999993, 1001.2114599999993, 1001.2380639999993, 1001.2808689999995, 1001.3632789999996, 1001.3999409999994, 1001.4106029999994]\n[999.9999999999994, 1000.1934979999995, 1000.2803799999991, 1000.5057419999991, 1000.5776239999993, 1000.9211489999997, 1001.5364453333325, 1001.8930102499993, 1001.1325689999994, 1001.139020624999, 1000.9070721249992, 1000.8263457499995, 1000.9257776666662, 1000.9932257499993, 1000.9682749999993, 1000.7770236249993, 1001.0818898749992, 1001.0142942499996, 1001.2327013333324, 1001.4262712499992, 1000.4908639999994, 1000.6756686249994, 1000.7202881249992, 1000.8025877499994, 1000.682952666666, 1000.6366517499993, 1000.4858139999994, 1000.5471696249992, 1000.6130811249993, 1000.6303047499991, 999.9624896666661, 1000.7125872499993, 1000.6654069999995, 1000.6625773749993, 1000.1756741249992, 1000.8540352499992, 1000.8565523333328, 1001.1351292499994, 1001.1336869999992, 1000.2863486249994, 1000.3059021249993, 1000.3888857499994, 1000.4372546666659, 1000.6295577499993, 1001.1868369999992, 1000.7245846249992, 1000.6864301249996, 1000.7207627499993, 1000.6942416666659, 1001.6657942499993, 1000.7650619999994, 1000.8488656249995, 1000.9819671249995, 1001.0109407499995, 1001.8601593333327]\n","output_type":"stream"}]},{"cell_type":"code","source":"result=[]\n\nfor i in range(len(index_list)):\n    ticks_per_episode = len(index_list[i])/dt-1-lag\n    def shm_market_gen():\n        return market_gen(gen=user_sub(df_new,index_list[i],start_trend=amplifier),\n                      lag=1)\n\n    env = Market(shm_market_gen,\n             lag=lag,\n             nstocks=1,\n             episode_length=ticks_per_episode)\n\n    agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n                        action_size=3,\n                       )\n\n    start_time = time.time()\n    print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n    for e in range(N_EPISODES):\n        agent.run_episode()\n        agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n    elapsed_time = time.time() - start_time\n    print(\"\\nTrain time: \", elapsed_time)\n    env.reset()\n    z = agent.view()\n\n    df = pd.DataFrame(z)\n    df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n    df['lastmarket']+=1\n    df['newmarket']+=1\n    df['short'] = np.nan\n    df.loc[df['action']==0, 'short'] = df['newmarket']\n    df['flat'] = np.nan\n    df.loc[df['action']==1, 'flat'] = df['newmarket']\n    df['long'] = np.nan\n    df.loc[df['action']==2, 'long'] = df['newmarket']\n    df['totalreward'] = df['reward'].cumsum()\n    df.to_csv('df_list_{}.csv'.format(i))\n    prob=agent.predict_model.predict(agent.state_memory)\n    action = np.random.choice(agent.action_space, p=prob[0])\n    pick_index=np.array(df_new['stock'][index_list[i]])\n    mean_diff=np.mean(np.absolute(np.diff(arr)))\n    action_arr=np.array(df['action']-1)\n    #pick_index=np.array(df_new['stock'][index_list[i]])\n    wap_future_index=[]\n    #assume in the first time index of t+60 the same as t\n    wap_future_index.append(pick_index[0])\n    for j in range(len(action_arr)):\n        wap_future_index.append(action_arr[j]*mean_diff+pick_index[j+1])\n    wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n    seconds_in_bucket=[j * 10 for j in index_list[i]]\n    comb=list(zip(seconds_in_bucket,pick_index,wap_future_index))\n    result.append(comb)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:26:57.702707Z","iopub.execute_input":"2023-11-20T06:26:57.703487Z","iopub.status.idle":"2023-11-20T06:31:02.256008Z","shell.execute_reply.started":"2023-11-20T06:26:57.703443Z","shell.execute_reply":"2023-11-20T06:31:02.254816Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"layer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_113\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:26:57\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:27:40 episode 1500/1500:, score: 1.5570000000000164, 10-episode avg: 1.6 Memory: 971.3 KB          \nTrain time:  42.303003787994385\nprev mkt: -2.5700 action: 0, new mkt -2.9830, reward 0.413000\nprev mkt: -2.9830 action: 0, new mkt -3.0290, reward 0.046000\nprev mkt: -3.0290 action: 0, new mkt -2.8450, reward -0.184000\nprev mkt: -2.8450 action: 0, new mkt -2.6850, reward -0.160000\nprev mkt: -2.6850 action: 0, new mkt -2.1530, reward -0.532000\nprev mkt: -2.1530 action: 0, new mkt -2.1790, reward 0.026000\nprev mkt: -2.1790 action: 0, new mkt -2.2350, reward 0.056000\nprev mkt: -2.2350 action: 0, new mkt -4.1270, reward 1.892000\n1.5570000000000164\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_115\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:27:40\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:28:18 episode 1500/1500:, score: 2.144000000000119, 10-episode avg: 2.9 Memory: 979.5 KB           \nTrain time:  38.446192264556885\nprev mkt: -2.6870 action: 0, new mkt -3.1120, reward 0.425000\nprev mkt: -3.1120 action: 2, new mkt -2.6890, reward 0.423000\nprev mkt: -2.6890 action: 0, new mkt -3.3760, reward 0.687000\nprev mkt: -3.3760 action: 2, new mkt -4.3590, reward -0.983000\nprev mkt: -4.3590 action: 2, new mkt -2.0660, reward 2.293000\nprev mkt: -2.0660 action: 0, new mkt -2.1070, reward 0.041000\nprev mkt: -2.1070 action: 0, new mkt -2.2110, reward 0.104000\n2.9899999999998954\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_117\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:28:18\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:28:57 episode 1500/1500:, score: 2.2650000000001, 10-episode avg: 1.9 Memory: 985.9 KB              \nTrain time:  38.29558444023132\nprev mkt: -2.7910 action: 2, new mkt -2.7650, reward 0.026000\nprev mkt: -2.7650 action: 0, new mkt -2.4960, reward -0.269000\nprev mkt: -2.4960 action: 2, new mkt -3.3670, reward -0.871000\nprev mkt: -3.3670 action: 2, new mkt -4.3430, reward -0.976000\nprev mkt: -4.3430 action: 2, new mkt -2.1530, reward 2.190000\nprev mkt: -2.1530 action: 0, new mkt -2.3640, reward 0.211000\nprev mkt: -2.3640 action: 2, new mkt -2.1520, reward 0.212000\n0.5230000000003656\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_119\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:28:57\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:29:39 episode 1500/1500:, score: 1.2269999999999754, 10-episode avg: 1.2 Memory: 991.6 KB           \nTrain time:  42.19053816795349\nprev mkt: -2.8310 action: 1, new mkt -2.9170, reward 0.000000\nprev mkt: -2.9170 action: 1, new mkt -3.2070, reward 0.000000\nprev mkt: -3.2070 action: 0, new mkt -3.5980, reward 0.391000\nprev mkt: -3.5980 action: 2, new mkt -4.3490, reward -0.751000\nprev mkt: -4.3490 action: 2, new mkt -2.2620, reward 2.087000\nprev mkt: -2.2620 action: 0, new mkt -2.2860, reward 0.024000\nprev mkt: -2.2860 action: 0, new mkt -2.3020, reward 0.016000\n1.7670000000000528\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_121\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"None\nStart: 06:29:39\n06:30:21 episode 1500/1500:, score: 2.7239999999999327, 10-episode avg: 3.5 Memory: 997.2 KB          \nTrain time:  42.031752586364746\nprev mkt: -2.7660 action: 0, new mkt -3.0740, reward 0.308000\nprev mkt: -3.0740 action: 2, new mkt -2.6750, reward 0.399000\nprev mkt: -2.6750 action: 0, new mkt -3.9490, reward 1.274000\nprev mkt: -3.9490 action: 2, new mkt -4.3040, reward -0.355000\nprev mkt: -4.3040 action: 2, new mkt -2.2400, reward 2.064000\nprev mkt: -2.2400 action: 0, new mkt -2.2370, reward -0.003000\nprev mkt: -2.2370 action: 0, new mkt -2.4020, reward 0.165000\n3.852000000000089\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_123\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 06:30:21\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"06:31:02 episode 1500/1500:, score: 1.377999999999929, 10-episode avg: 5.0 Memory: 1004.3 KB             \nTrain time:  40.34423565864563\nprev mkt: -2.7870 action: 2, new mkt -3.0740, reward -0.287000\nprev mkt: -3.0740 action: 0, new mkt -2.7020, reward -0.372000\nprev mkt: -2.7020 action: 0, new mkt -3.2500, reward 0.548000\nprev mkt: -3.2500 action: 0, new mkt -4.3570, reward 1.107000\nprev mkt: -4.3570 action: 2, new mkt -2.1920, reward 2.165000\nprev mkt: -2.1920 action: 0, new mkt -2.3410, reward 0.149000\nprev mkt: -2.3410 action: 2, new mkt -4.1650, reward -1.824000\n1.4860000000001037\n","output_type":"stream"}]},{"cell_type":"code","source":"\nresult_list = list(chain.from_iterable(result))\n#print(result_list)\nsorted_result = sorted(result_list, key=lambda x: x[0])\n#print(sorted_result)\nseconds_in_bucket, wap, wap_future = zip(*sorted_result)\nprint(list(seconds_in_bucket))\nprint(list(wap))\nprint(list(wap_future))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.257567Z","iopub.execute_input":"2023-11-20T06:31:02.258503Z","iopub.status.idle":"2023-11-20T06:31:02.266168Z","shell.execute_reply.started":"2023-11-20T06:31:02.258468Z","shell.execute_reply":"2023-11-20T06:31:02.265144Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540]\n[1000.0, 997.378, 997.955, 997.869, 997.284, 997.46, 997.4300000000001, 997.313, 997.2090000000001, 997.169, 997.2339999999999, 997.213, 997.017, 996.888, 997.235, 997.0830000000001, 996.9259999999999, 996.9259999999999, 996.971, 997.3109999999999, 997.5039999999999, 996.793, 997.325, 997.298, 997.155, 996.6239999999999, 996.633, 996.402, 996.051, 996.75, 997.3149999999999, 995.641, 995.657, 995.651, 995.696, 995.6429999999999, 997.8470000000001, 997.934, 997.8470000000001, 997.738, 997.76, 997.808, 997.8209999999999, 997.893, 997.636, 997.7139999999999, 997.7629999999999, 997.659, 997.765, 997.789, 997.848, 997.698, 997.598, 995.835, 995.873]\n[1000.0, 997.378, 997.955, 997.869, 997.284, 997.46, 996.7767777777779, 996.685375, 997.8966250000001, 997.169, 996.6567499999999, 998.050375, 996.3637777777778, 997.515625, 996.547375, 997.0830000000001, 997.50325, 996.0886249999999, 996.3177777777778, 996.683375, 998.1916249999999, 996.249875, 996.74775, 996.4606249999999, 996.5017777777778, 997.2516249999999, 997.3206250000001, 996.9451250000001, 996.6282500000001, 995.9126249999999, 996.6617777777777, 996.2686249999999, 996.3446250000001, 996.194125, 996.2732500000001, 996.480375, 997.1937777777779, 997.306375, 997.1593750000001, 997.194875, 997.1827499999999, 996.9706249999999, 997.1677777777777, 997.2653750000001, 998.323625, 997.1708749999999, 997.1857499999999, 998.4963750000001, 997.1117777777778, 997.161375, 998.535625, 997.154875, 998.17525, 994.997625, 995.2197777777778]\n","output_type":"stream"}]},{"cell_type":"code","source":"#agent.rlplot(\"Training Progress: stock {} date {}\".format(int(stock),int(date)))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.267415Z","iopub.execute_input":"2023-11-20T06:31:02.267765Z","iopub.status.idle":"2023-11-20T06:31:02.281289Z","shell.execute_reply.started":"2023-11-20T06:31:02.267736Z","shell.execute_reply":"2023-11-20T06:31:02.279976Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"#env.reset()\n#z = agent.view()\n\n#df = pd.DataFrame(z)\n#df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n#df['lastmarket']+=1\n#df['newmarket']+=1\n#df['short'] = np.nan\n#df.loc[df['action']==0, 'short'] = df['newmarket']\n#df['flat'] = np.nan\n#df.loc[df['action']==1, 'flat'] = df['newmarket']\n#df['long'] = np.nan\n#df.loc[df['action']==2, 'long'] = df['newmarket']\n#df['totalreward'] = df['reward'].cumsum()\n#df.to_csv('df.csv')\n#df","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.282618Z","iopub.execute_input":"2023-11-20T06:31:02.282949Z","iopub.status.idle":"2023-11-20T06:31:02.291755Z","shell.execute_reply.started":"2023-11-20T06:31:02.282920Z","shell.execute_reply":"2023-11-20T06:31:02.290864Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"def tradesim_chart(df, title=\"Trading Simulation\"):\n\n    fig = go.Figure()\n    markersize=4\n\n    # x axis\n    x = df['timestep']\n\n    red = 'rgba(192, 32, 32, 0.75)'\n    blue = 'rgba(32, 32, 192, 0.75)'\n    green = 'rgba(0, 204, 0, 0.75)'\n    black = 'rgba(32, 32, 32, 0.75)'\n\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n    fig.add_trace(go.Scatter(y=df['short'],\n                             x=x,\n                             name='Short (left axis)',\n                             mode='markers',\n                             marker=dict(size=markersize,\n                                         color=red),\n                            ),\n                  secondary_y=False,\n                 )\n\n    fig.add_trace(go.Scatter(y=df['flat'],\n                             x=x,\n                             name='Flat (left axis)',\n                             mode='markers',\n                             marker=dict(size=markersize,\n                                         color=blue),\n                            ),\n                  secondary_y=False,\n                 )\n\n    fig.add_trace(go.Scatter(y=df['long'],\n                             x=x,\n                             name='Long (left axis)',\n                             mode='markers',\n                             marker=dict(size=markersize,\n                                         color=green),\n                            ),\n                  secondary_y=False,\n                 )\n\n    fig.add_trace(go.Scatter(y=df['totalreward'],\n                             x=x,\n                             name='Total reward (right)',\n                             mode='markers',\n                             marker=dict(size=markersize,\n                                         color=black),\n                            ),\n                  secondary_y=True,\n                 )\n\n    # plot attributes\n    fig.update_layout(\n        title= dict(text=title,\n                    x=0.5,\n                    xanchor='center'),\n        xaxis=dict(\n            title=\"Timesteps\",\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        yaxis=dict(\n            title=\"Price\",\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        showlegend=True,\n        legend=dict(x=0.738, y=0.05)\n    )\n\n    fig.update_yaxes(title_text=\"Total reward\", secondary_y=True)\n\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.293362Z","iopub.execute_input":"2023-11-20T06:31:02.293638Z","iopub.status.idle":"2023-11-20T06:31:02.307678Z","shell.execute_reply.started":"2023-11-20T06:31:02.293613Z","shell.execute_reply":"2023-11-20T06:31:02.306783Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"#tradesim_chart(df, title=\"Trading stock {} date {}\".format(int(stock),int(date)))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.308755Z","iopub.execute_input":"2023-11-20T06:31:02.309117Z","iopub.status.idle":"2023-11-20T06:31:02.319784Z","shell.execute_reply.started":"2023-11-20T06:31:02.309088Z","shell.execute_reply":"2023-11-20T06:31:02.318806Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"#prob=agent.predict_model.predict(agent.state_memory)\n#print(prob)\n#action = np.random.choice(agent.action_space, p=prob[0])\n#print(action)\n#action\n#arr=np.array(df_new['stock'][index_pick])\n#mean_diff=np.mean(np.absolute(np.diff(arr)))\n#action_arr=np.array(df['action']-1)\n#print(action_arr)\n#pick=np.array(df_new['stock'][index_pick])\n#wap_future=[]\n#assume the first stock i\n#wap_future.append(pick[0])\n#for i in range(len(action_arr)):\n#    wap_future.append(action_arr[i]*mean_diff+pick[i+1])\n#index=[1,2,3,4,5,6,7,8,9]\n#wap_future.append((action-1)*mean_diff+pick[-1])\n#plt.plot(pick,\"*\",label=\"truth\")\n#plt.plot(index,wap_future,\"*\",label=\"prediction\")\n#plt.legend()\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.323605Z","iopub.execute_input":"2023-11-20T06:31:02.323991Z","iopub.status.idle":"2023-11-20T06:31:02.332598Z","shell.execute_reply.started":"2023-11-20T06:31:02.323956Z","shell.execute_reply":"2023-11-20T06:31:02.331348Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def shm_market_gen():\n#    return market_gen(gen=user_sub(df_index,index_pick,start_trend=amplifier),\n#                      lag=lag)\n\n#env = Market(shm_market_gen,\n#             lag=lag,\n#             nstocks=1,\n#             episode_length=ticks_per_episode)\n\n#agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n#                        action_size=3,\n#                       )\n#agent.reset()\n#start_time = time.time()\n#print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n#for e in range(N_EPISODES):\n#    agent.run_episode()\n#    agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n#elapsed_time = time.time() - start_time\n#print(\"\\nTrain time: \", elapsed_time)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.334041Z","iopub.execute_input":"2023-11-20T06:31:02.334340Z","iopub.status.idle":"2023-11-20T06:31:02.342416Z","shell.execute_reply.started":"2023-11-20T06:31:02.334315Z","shell.execute_reply":"2023-11-20T06:31:02.341573Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"#agent.rlplot(\"Training Progress: Index date {}\".format(int(date)))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.344100Z","iopub.execute_input":"2023-11-20T06:31:02.344443Z","iopub.status.idle":"2023-11-20T06:31:02.356336Z","shell.execute_reply.started":"2023-11-20T06:31:02.344413Z","shell.execute_reply":"2023-11-20T06:31:02.355316Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"#prob=agent.predict_model.predict(agent.state_memory)\n#print(prob)\n#action = np.random.choice(agent.action_space, p=prob[0])\n#print(action)\n#action\n#arr=np.array(df_index['stock'][index_pick])\n#mean_diff=np.mean(np.absolute(np.diff(arr)))\n#print(np.mean(np.absolute(np.diff(arr))))\n#action_arr=np.array(df['action']-1)\n#print(action_arr)\n#pick_index=np.array(df_index['stock'][index_pick])\n#wap_future_index=[]\n#assume in the first time index of t+60 the same as t\n#wap_future_index.append(pick_index[0])\n#for i in range(len(action_arr)):\n#    wap_future_index.append(action_arr[i]*mean_diff+pick_index[i+1])\n#index=[1,2,3,4,5,6,7,8,9]\n#wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n#plt.plot(pick_index,\"*\",label=\"truth\")\n#plt.plot(index,wap_future_index,\"*\",label=\"prediction\")\n#plt.legend()\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.357824Z","iopub.execute_input":"2023-11-20T06:31:02.358130Z","iopub.status.idle":"2023-11-20T06:31:02.367471Z","shell.execute_reply.started":"2023-11-20T06:31:02.358103Z","shell.execute_reply":"2023-11-20T06:31:02.366460Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"#def target():\n#    target_out=[]\n#    for i in range(len(index_pick)):\n#        target_out.append((wap_future[i]/pick[i]-wap_future_index[i]/pick_index[i])*10000)\n#    return target_out\n\n#seconds_in_bucket=[i * 10 for i in index_pick]\n#comb=list(zip(seconds_in_bucket,target()))\n#print(\"day=\",date,\"Stock=\",stock,\"seconds_in_bucket\", seconds_in_bucket, target())\n#print(comb)        \n#sorted_zipped = sorted(comb, key=lambda x: x[0])    \n#sorted_list1, sorted_list2 = zip(*sorted_zipped)\n#print(list(sorted_list1))\n#print(list(sorted_list2))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.368580Z","iopub.execute_input":"2023-11-20T06:31:02.368916Z","iopub.status.idle":"2023-11-20T06:31:02.378543Z","shell.execute_reply.started":"2023-11-20T06:31:02.368887Z","shell.execute_reply":"2023-11-20T06:31:02.377525Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"#env = Market(shm_market_gen,\n#             lag=lag,\n#             nstocks=1,\n#             episode_length=ticks_per_episode)\n#env.reset()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.379769Z","iopub.execute_input":"2023-11-20T06:31:02.380181Z","iopub.status.idle":"2023-11-20T06:31:02.392245Z","shell.execute_reply.started":"2023-11-20T06:31:02.380151Z","shell.execute_reply":"2023-11-20T06:31:02.391141Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"#import optiver2023\n#env = optiver2023.make_env()\n#iter_test = env.iter_test()\n#counter = 0\n#for (test, revealed_targets, sample_prediction) in iter_test:\n    #feat = generate_features(test)\n    \n    #sample_prediction['target'] \n#    env.predict(sample_prediction)\n#    counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T06:31:02.394074Z","iopub.execute_input":"2023-11-20T06:31:02.394406Z","iopub.status.idle":"2023-11-20T06:31:03.262027Z","shell.execute_reply.started":"2023-11-20T06:31:02.394378Z","shell.execute_reply":"2023-11-20T06:31:03.260797Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"}]}]}