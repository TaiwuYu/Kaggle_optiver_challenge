{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T17:59:56.421973Z","iopub.execute_input":"2023-11-20T17:59:56.422211Z","iopub.status.idle":"2023-11-20T17:59:56.762848Z","shell.execute_reply.started":"2023-11-20T17:59:56.422188Z","shell.execute_reply":"2023-11-20T17:59:56.761976Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"#Instal new version of Tensorflow\n#!pip3 install --upgrade tensorflow==2.12.0","metadata":{"execution":{"iopub.status.busy":"2023-11-20T17:59:56.764460Z","iopub.execute_input":"2023-11-20T17:59:56.764869Z","iopub.status.idle":"2023-11-20T17:59:56.768995Z","shell.execute_reply.started":"2023-11-20T17:59:56.764842Z","shell.execute_reply":"2023-11-20T17:59:56.768059Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\nfrom datetime import datetime\nimport time\nimport resource\nimport pickle\nimport os\nimport pdb\n\n\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom scipy.interpolate import pchip_interpolate\n\nimport tensorflow as tf\n#import tensorflow.keras\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout\n#In old version of Tensorflow 2.8.0\n#from tensorflow.keras.optimizers import Adam\n#In new version of Tensorflow 2.12.0, please use legacy to import Adam\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.regularizers import l2\nimport tensorflow.keras.backend as K\n#from keras import backend as K\n\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom IPython.display import clear_output, display, HTML\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# originally built on old TensorFlow and Keras which didn't support eager execution\ntf.compat.v1.disable_eager_execution()\n#tf.compat.v1.disable_v2_behavior()\n#tf.compat.v1.enable_eager_execution()\n\n# set seeds for reproducibility\n# np.random.uniform(0,10000) 4465\nrandom.seed(4465)\nnp.random.seed(4465)\ntf.random.set_seed(4465)\n\nprint(\"TensorFlow %s\" % tf.__version__)\nprint(\"Keras %s\" % keras.__version__)\nprint(\"plotly %s\" % plotly.__version__)\nprint(\"pandas %s\" % pd.__version__)\nprint(\"numpy %s\" % np.__version__)\n\n# If model save directory isn't made yet, make it\nif not os.path.exists('model_output'):\n    os.makedirs('model_output')\nif not os.path.exists('model_output/trading'):\n    os.makedirs('model_output/trading')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T17:59:56.770241Z","iopub.execute_input":"2023-11-20T17:59:56.770563Z","iopub.status.idle":"2023-11-20T18:00:05.806622Z","shell.execute_reply.started":"2023-11-20T17:59:56.770531Z","shell.execute_reply":"2023-11-20T18:00:05.805745Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow 2.12.0\nKeras 2.12.0\nplotly 5.15.0\npandas 2.0.3\nnumpy 1.23.5\n","output_type":"stream"}]},{"cell_type":"code","source":"def sizeof_fmt(num, suffix='B'):\n    \"\"\"given memory as int format as memory units eg KB\"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Y', suffix)\n\ndef memusage():\n    \"\"\"print memory usage\"\"\"\n    return sizeof_fmt(int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n\nmemusage()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:05.809352Z","iopub.execute_input":"2023-11-20T18:00:05.809963Z","iopub.status.idle":"2023-11-20T18:00:05.819595Z","shell.execute_reply.started":"2023-11-20T18:00:05.809936Z","shell.execute_reply":"2023-11-20T18:00:05.818635Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'691.3 KB'"},"metadata":{}}]},{"cell_type":"code","source":"def make_figure(*series, title=\"\", xtitle=\"\", ytitle=\"\"):\n    fig = go.Figure()\n    series=list(series)\n    x = series.pop(0)\n    for s in series:\n        fig.add_trace(go.Scatter(y=s, x=x))\n    fig.update_layout(\n        title= dict(text=title,\n                    x=0.5,\n                    xanchor='center'),\n        xaxis=dict(\n            title=xtitle,\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        yaxis=dict(\n            title=ytitle,\n            linecolor='black',\n            linewidth=1,\n            mirror=True\n        ),\n        showlegend=False\n    )\n\n    return fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:05.820722Z","iopub.execute_input":"2023-11-20T18:00:05.820981Z","iopub.status.idle":"2023-11-20T18:00:05.833113Z","shell.execute_reply.started":"2023-11-20T18:00:05.820958Z","shell.execute_reply":"2023-11-20T18:00:05.832372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"stock_series = []\nprob_memory = []\n\namplifier=1000\ndate=478\nstock=100\n#be cautious when setting dt. dt is inverse proportional to learning time dt=1 means no optimization\ndt=1\n\n#filename=\"IndexWap_day231.csv\"\n#excelfilereader=pd.read_csv(r'/content/kaggle_optiver_data/IndexWap_day231.csv', header=None)\n#data = excelfilereader.iloc[0,:].values.tolist()\n#data = excelfilereader.iloc[0,:]*1000\n\n#excelfilereader=pd.read_csv(r'/kaggle/input/optiver-trading-at-the-close/train.csv')\nexcelfilereader=pd.read_csv(r'/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv')\ndata_sub=excelfilereader[excelfilereader['stock_id']==stock]\ndata_sub2=data_sub[data_sub['date_id']==date]\ndata=pd.DataFrame(data_sub2,columns=['wap'])\ndata = data.values.tolist()\ndata=np.squeeze(data)\ndata=data*amplifier\n\n\nstock_series=data\ntime_series=range(0,data.shape[0])\ndf = pd.DataFrame({'dateindex': time_series, 'stock': stock_series})\n\n\n\nmake_figure(df['dateindex'], df['stock'],\n            title=\"Stock {} price at date {}\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\nx=df['dateindex']\ny=df['stock']\nx_sub = np.linspace(min(x), max(x), num=int(df.shape[0]/dt))\ny_sub = pchip_interpolate(x, y, x_sub)\ndf_new = pd.DataFrame({'dateindex': x_sub, 'stock': y_sub})\n\nmake_figure(df_new['dateindex'], df_new['stock'],\n            title=\"Stock {} price at date {} after interpolation\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:05.834227Z","iopub.execute_input":"2023-11-20T18:00:05.834539Z","iopub.status.idle":"2023-11-20T18:00:06.269396Z","shell.execute_reply.started":"2023-11-20T18:00:05.834509Z","shell.execute_reply":"2023-11-20T18:00:06.268497Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"64a2f152-98d1-4265-bcf3-730ac80961b7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"64a2f152-98d1-4265-bcf3-730ac80961b7\")) {                    Plotly.newPlot(                        \"64a2f152-98d1-4265-bcf3-730ac80961b7\",                        [{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54],\"y\":[1000.0,997.378,997.955,997.869,997.284,997.46,997.4300000000001,997.313,997.2090000000001,997.169,997.2339999999999,997.213,997.017,996.888,997.235,997.0830000000001,996.9259999999999,996.9259999999999,996.971,997.3109999999999,997.5039999999999,996.793,997.325,997.298,997.155,996.6239999999999,996.633,996.402,996.051,996.75,997.3149999999999,995.641,995.657,995.651,995.696,995.6429999999999,997.8470000000001,997.934,997.8470000000001,997.738,997.76,997.808,997.8209999999999,997.893,997.636,997.7139999999999,997.7629999999999,997.659,997.765,997.789,997.848,997.698,997.598,995.835,995.873],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('64a2f152-98d1-4265-bcf3-730ac80961b7');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"ce55dfd6-3d28-456b-96e5-d8f2087bdd5f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ce55dfd6-3d28-456b-96e5-d8f2087bdd5f\")) {                    Plotly.newPlot(                        \"ce55dfd6-3d28-456b-96e5-d8f2087bdd5f\",                        [{\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0],\"y\":[1000.0,997.378,997.955,997.869,997.284,997.46,997.4300000000001,997.313,997.2090000000001,997.169,997.2339999999999,997.213,997.017,996.888,997.235,997.0830000000001,996.9259999999999,996.9259999999999,996.971,997.3109999999999,997.5039999999999,996.793,997.325,997.298,997.155,996.6239999999999,996.633,996.402,996.051,996.75,997.3149999999999,995.641,995.657,995.651,995.696,995.6429999999999,997.8470000000001,997.934,997.8470000000001,997.738,997.76,997.808,997.8209999999999,997.893,997.636,997.7139999999999,997.7629999999999,997.659,997.765,997.789,997.848,997.698,997.598,995.835,995.873],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478 after interpolation\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('ce55dfd6-3d28-456b-96e5-d8f2087bdd5f');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"index_1=[0,6,12,18,24,30,36,42,48,54]\nindex_2=[1,7,13,19,25,31,37,43,49]\nindex_3=[2,8,14,20,26,32,38,44,50]\nindex_4=[3,9,15,21,27,33,39,45,51]\nindex_5=[4,10,16,22,28,34,40,46,52]\nindex_6=[5,11,17,23,29,35,41,47,53]\nindex_list=[index_1,index_2,index_3,index_4,index_5, index_6]\n#index_list\nmake_figure(df_new['dateindex'][index_6], df_new['stock'][index_6],\n            title=\"Stock {} price at date {} sub 1\".format(int(stock), int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.270642Z","iopub.execute_input":"2023-11-20T18:00:06.271178Z","iopub.status.idle":"2023-11-20T18:00:06.292554Z","shell.execute_reply.started":"2023-11-20T18:00:06.271151Z","shell.execute_reply":"2023-11-20T18:00:06.291385Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"40d0354b-5eb1-4a75-be16-10c9033dbf76\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"40d0354b-5eb1-4a75-be16-10c9033dbf76\")) {                    Plotly.newPlot(                        \"40d0354b-5eb1-4a75-be16-10c9033dbf76\",                        [{\"x\":[5.0,11.0,17.0,23.0,29.0,35.0,41.0,47.0,53.0],\"y\":[997.46,997.213,996.9259999999999,997.298,996.75,995.6429999999999,997.808,997.659,995.835],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Stock 100 price at date 478 sub 1\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('40d0354b-5eb1-4a75-be16-10c9033dbf76');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"def indexWAP(data, date, t):\n    coefficient = [0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.002,\n                   0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004, 0.002, 0.002, 0.004, 0.002,\n                   0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004, 0.004, 0.004, 0.006, 0.002, 0.002, 0.04,\n                   0.002, 0.002, 0.004, 0.04, 0.002, 0.001, 0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002,\n                   0.006, 0.004, 0.006, 0.004, 0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004,\n                   0.002, 0.004, 0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n                   0.02, 0.004, 0.006, 0.002, 0.02, 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02, 0.006, 0.001, 0.002,\n                   0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002, 0.004, 0.006, 0.006, 0.001, 0.04,\n                   0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002, 0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008,\n                   0.006, 0.004, 0.002, 0.006, 0.002, 0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008,\n                   0.006, 0.008, 0.002, 0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001,\n                   0.002, 0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002, 0.04,\n                   0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02, 0.004, 0.002, 0.006, 0.02, 0.001, 0.002, 0.006,\n                   0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04, 0.002, 0.008, 0.002, 0.004, 0.001,\n                   0.004, 0.006, 0.004]\n    date_sub = data.loc[(data[\"date_id\"] == date) & (data[\"seconds_in_bucket\"] == t)]\n    stk_id = pd.DataFrame(date_sub, columns=[\"stock_id\"])\n    wap = pd.DataFrame(date_sub, columns=[\"wap\"])\n    stk_id_arr = np.array(stk_id).reshape(-1)\n\n    missing_elemnts = [item for item in range(stk_id_arr[0], 200) if item not in stk_id_arr]\n    missing_elemnts_arr = np.array(missing_elemnts).reshape(-1)\n    list_coefficient = coefficient\n    for i in sorted(missing_elemnts_arr, reverse=True):\n        list_coefficient.pop(i)\n\n    norm = [float(i) / sum(list_coefficient) for i in list_coefficient]\n    stock_index = np.dot(norm, wap)\n    return (np.array([stock_index]).item())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.294037Z","iopub.execute_input":"2023-11-20T18:00:06.294407Z","iopub.status.idle":"2023-11-20T18:00:06.310516Z","shell.execute_reply.started":"2023-11-20T18:00:06.294356Z","shell.execute_reply":"2023-11-20T18:00:06.309673Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"time_arr = np.arange(0, 550, 10)\ndate_0 = excelfilereader.loc[(excelfilereader[\"date_id\"] == date)]\n\nindex = []\n\nfor i in time_arr:\n        index.append(indexWAP(date_0, date, i))\n\nindex=np.squeeze(index)\nindex=index*amplifier\ndf_index = pd.DataFrame({'dateindex': x_sub, 'stock': index})\n\nmake_figure(df_index['dateindex'], df_index['stock'],\n            title=\"Index at date {}\".format(int(date)),\n            xtitle='Timesteps',\n            ytitle='Value'\n           )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.311492Z","iopub.execute_input":"2023-11-20T18:00:06.311754Z","iopub.status.idle":"2023-11-20T18:00:06.493512Z","shell.execute_reply.started":"2023-11-20T18:00:06.311732Z","shell.execute_reply":"2023-11-20T18:00:06.492650Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"67bfeeda-b52c-4ba9-b2a2-8e653f5cdf4a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"67bfeeda-b52c-4ba9-b2a2-8e653f5cdf4a\")) {                    Plotly.newPlot(                        \"67bfeeda-b52c-4ba9-b2a2-8e653f5cdf4a\",                        [{\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0],\"y\":[999.9999999999994,1000.1934979999995,1000.2803799999991,1000.5057419999994,1000.5776239999993,1000.9211489999994,1001.0868889999991,1001.4386759999993,1001.6055709999994,1001.571023999999,1001.2883839999993,1001.2153459999995,1001.3753339999994,1001.4475599999993,1001.4412769999994,1001.2090269999993,1000.7005779999994,1000.6252939999994,1000.7831449999993,1000.9719369999992,1000.9638659999994,1001.1076719999992,1001.1015999999993,1001.1915879999993,1001.1325089999993,1001.0909859999992,1000.9588159999994,1000.9791729999993,1000.9943929999994,1001.0193049999993,1000.4120459999995,1000.2582529999993,1000.1924049999996,1000.2305739999993,1000.5569859999993,1000.4650349999995,1000.4069959999995,1000.6807949999994,1000.6606849999995,1000.7183519999994,1000.6872139999991,1000.7778859999994,1000.8868109999993,1001.0838919999993,1001.1868369999992,1001.1565879999993,1001.0677419999996,1001.1097629999992,1001.1437979999995,1001.2114599999991,1001.2380639999993,1001.2808689999993,1001.3632789999994,1001.3999409999994,1001.4106029999994],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Index at date 478\",\"x\":0.5,\"xanchor\":\"center\"},\"xaxis\":{\"title\":{\"text\":\"Timesteps\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"yaxis\":{\"title\":{\"text\":\"Value\"},\"linecolor\":\"black\",\"linewidth\":1,\"mirror\":true},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('67bfeeda-b52c-4ba9-b2a2-8e653f5cdf4a');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"def user_gen(df,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    while t<=(df.shape[0]):\n        stock_price= df['stock'][t]\n        yield(t, stock_price, trend_index)\n        t+=1\n\ndef user_gen_dt(df,dt=1,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    x=df['dateindex']\n    y=df['stock']\n    x_sub = np.linspace(min(x), max(x), num=int(df.shape[0]/dt))\n  #print(np.shape(x_sub))\n    y_sub = pchip_interpolate(x, y, x_sub)\n  #print(np.shape(y_sub))\n    while t<=int(len(x_sub)/dt):\n        stock_price= y_sub[t]\n        yield(t, stock_price, trend_index)\n        t+=dt\n\ndef user_sub(df,index_sub,start_trend=amplifier):\n    t=0\n    trend_index = start_trend\n    x=range(len(index_sub))\n    y=[]\n    #x=df['dateindex'][index_sub]\n    y=df['stock'][index_sub]\n    for i in range(len(index_sub)):\n        y[i]=df['stock'][index_sub[i]]\n    while t<=int(len(x)):\n        stock_price= y[t]\n        yield(t, stock_price, trend_index)\n        t+=1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.497031Z","iopub.execute_input":"2023-11-20T18:00:06.497304Z","iopub.status.idle":"2023-11-20T18:00:06.506489Z","shell.execute_reply.started":"2023-11-20T18:00:06.497280Z","shell.execute_reply":"2023-11-20T18:00:06.505503Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def market_gen(gen, lag=2):\n\n    buffer = []\n    diffbuffer = []\n\n\n    # fill buffer\n    dt, last, trend = next(gen)\n    for i in range(lag):\n        prev = last\n        dt, last, trend = next(gen)\n        buffer.append(last-trend)\n        diffbuffer.append(last-prev)\n\n    # yield first group of lag vals and diffs\n    yield buffer+diffbuffer\n\n    while(True):\n        prev = last\n        dt, last, trend = next(gen)\n        buffer.pop(0)\n        buffer.append(last-trend)\n        diffbuffer.pop(0)\n        diffbuffer.append(last-prev)\n        yield buffer+diffbuffer\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.507563Z","iopub.execute_input":"2023-11-20T18:00:06.507854Z","iopub.status.idle":"2023-11-20T18:00:06.519914Z","shell.execute_reply.started":"2023-11-20T18:00:06.507831Z","shell.execute_reply":"2023-11-20T18:00:06.519058Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Market:\n    \"\"\"Follows OpenAI gym environment convention basically\n    init with generator and number of stocks\n    reset() - generate and return first state\n    step() - generate next state and reward\n    \"\"\"\n    def __init__(self, gen, lag=16, nstocks=1, episode_length=300):\n        self.genfunc = gen\n        self.nstocks = nstocks\n        self.episode_length = episode_length\n        self.t = 0\n        self.total_reward = 0\n        self.lag = lag\n        self.observation_space = np.asarray([1] * nstocks * lag * 2,)\n        self.state_size = nstocks * lag * 2\n        self.action_size = 2\n\n    def reset(self):\n        self.t = 0\n        self.total_reward = 0\n        self.gen = [self.genfunc() for _ in range(self.nstocks)]\n        self.state=[next(g) for g in self.gen]\n        self.state = np.asarray([s for s in self.state])\n        return self.state\n\n    def render(self):\n        print(self.state[0, nstocks-1])\n\n    def step(self, action):\n        action = np.asarray([action])\n        try:\n            self.state=[next(g) for g in self.gen]\n        except StopIteration:\n            return print(\"generator failed.\\n\")\n        \n        self.state = np.asarray([s for s in self.state])\n        # last element is most recent change\n        stock_delta = np.asarray([s[-1] for s in self.state])\n        # element at lag-1 is most recent deviation\n        market_price = np.asarray([s[self.lag-1]+100 for s in self.state])\n        # map actions 0 1 2 to positions -1, 0, 1\n        position = action - 1\n        reward = position @ stock_delta\n        self.total_reward += reward\n        self.t += 1\n        done = True if self.episode_length and self.t >= self.episode_length else False\n        # state, reward, done, info\n        return self.state, reward, done, market_price\n\n    def close(self):\n        pass\n\n\n#env = Market(user_gen_dt(df,0.1), lag=1, nstocks=1, episode_length=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.521419Z","iopub.execute_input":"2023-11-20T18:00:06.521797Z","iopub.status.idle":"2023-11-20T18:00:06.534889Z","shell.execute_reply.started":"2023-11-20T18:00:06.521766Z","shell.execute_reply":"2023-11-20T18:00:06.534129Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"DISCOUNT_RATE = 0\n# WIN_REWARD = 10\nEPSILON_DECAY = 0.995\nSAMPLE_SIZE = 256\nRENDER = False\nOUTPUT_DIR = 'model_output/trading/'\n\nclass DQN_Agent:\n    def __init__(self, state_size, action_size, filename=\"dqn\",\n                 discount_rate=DISCOUNT_RATE,\n                 learning_rate=0.001,\n                 epsilon=1.0,\n                 epsilon_decay=EPSILON_DECAY,\n                 epsilon_min=0.01):\n\n        self.state_size = state_size\n        self.action_size = action_size\n        self.filename = filename\n        self.discount_rate = discount_rate\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.learning_rate = learning_rate\n\n        self.model = self.build_model()\n        self.memory = pd.DataFrame(columns=[\"state\", \"action\", \"next_state\",\n                                            \"reward\", \"done\"])\n        self.memory_size = 100000\n        self.results = []\n        self.train_batch_size = 1\n        self.timestep = 0\n        self.save_interval = 10\n\n    def build_model(self,\n                    n_hidden_layers=2,\n                    hidden_layer_size=16,\n                    activation='relu',\n                    reg_penalty=0.001,\n                    dropout=0.0675,\n                    verbose=True\n                   ):\n        \"\"\"return keras NN model per inputs\n        input is a state - array of size state_size\n        output is an array of action values - array of size action_size\n        \"\"\"\n\n        inputs = Input(shape=(self.state_size,), name=\"Input\")\n        last_layer = inputs\n\n        for i in range(n_hidden_layers):\n            if verbose:\n                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n                print(formatstr % (i + 1,\n                                   hidden_layer_size,\n                                   activation,\n                                   reg_penalty,\n                                   dropout))\n            # add dropout, but not on inputs, only between hidden layers\n            if i and dropout:\n                last_layer = Dropout(dropout, name=\"Dropout%02d\" % i)(last_layer)\n\n            last_layer = Dense(units=hidden_layer_size,\n                               activation=activation,\n                               kernel_initializer=glorot_uniform(),\n                               kernel_regularizer=l2(reg_penalty),\n                               name=\"Dense%02d\" % i)(last_layer)\n\n        outputs = Dense(self.action_size, activation='linear', name=\"Output\")(last_layer)\n\n        #model = Model(inputs=input_layer , output=last_layer)\n        model = Model(inputs=inputs, outputs=outputs)\n\n        if verbose:\n            print(model.summary())\n\n        model.compile(loss='mse', optimizer=Adam(\n            #learning_rate=self.learning_rate\n        ))\n\n        return model\n\n    def remember(self):\n        \"\"\"store the states and rewards needed to fit the model\"\"\"\n        # append in place\n        self.memory.loc[self.memory.shape[0]] = [self.state,\n                                                 self.action,\n                                                 self.next_state,\n                                                 self.reward,\n                                                 self.done]\n\n    def train(self):\n        \"\"\"train the model on experience stored by remember\"\"\"\n\n        # need at least SAMPLE_SIZE observations\n        if self.memory.shape[0] < SAMPLE_SIZE:\n            return\n\n        # truncate memory\n        self.memory = self.memory[-self.memory_size:]\n        # sample sample_size observations from memory\n        minibatch = self.memory.sample(n=SAMPLE_SIZE)\n\n        # target is our best estimate of value of each action\n        X_fit = np.concatenate(minibatch['state'].values)\n        X_fit = X_fit.reshape((SAMPLE_SIZE, self.state_size))\n        Y_pred = self.model.predict(X_fit)\n\n        # we don't just fit model against model's own prediction, gets us nowhere\n        # we improve the target by what we learned about the action we actually took\n        # value is reward obtained + predicted value of the observed next state\n        minibatch['target_observed'] = minibatch['reward']\n        # if done, target is the reward\n        # reward by gym env is only 1 for each timestep of survival\n        # but we also added a reward of -10 on failure\n        # if not done, add discount_rate  * Q-value prediction for  observed next state\n        not_done = minibatch.loc[minibatch['done'] == False]\n        X_observed = np.concatenate(not_done['next_state'].values)\n        X_observed = X_observed.reshape((not_done.shape[0], self.state_size))\n        # run all predictions at once\n        # iterates faster but does not train after each prediction\n        y_observed_pred = np.amax(self.model.predict(X_observed), axis=1)\n        minibatch.loc[minibatch['done'] == False, 'target_observed'] \\\n            += self.discount_rate * y_observed_pred\n        # vectorized vlookup - update col specified by action with target_observed\n        np.put_along_axis(Y_pred,\n                          minibatch['action'].astype(int).values.reshape(SAMPLE_SIZE, 1),\n                          minibatch['target_observed'].values.reshape(SAMPLE_SIZE, 1),\n                          axis=1)\n        # fit model against improved target\n        # arbitrary 8 batch size to reduce variance a little and speed up fit\n        self.model.fit(X_fit, Y_pred,\n                       epochs=1,\n                       batch_size=self.train_batch_size,\n                       verbose=0)\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def act(self, state):\n        \"\"\"pick an action using model\"\"\"\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])\n\n    def save(self):\n        \"save agent: pickle self and use Keras native save model\"\n        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n        self.model.save(\"%s.h5\" % fullname)\n        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n\n    def load(filename):\n        \"load saved agent\"\n        new = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n        new.model = load_model(\"%s.h5\" % filename)\n        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(new.results),\n                                                                      len(new.memory),\n                                                                      new.epsilon))\n        return new\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        self.total_reward = 0\n\n    def increment_time(self):\n        \"\"\"increment timestep counter\"\"\"\n        self.timestep += 1\n\n    def save_score(self):\n        \"\"\"save score of each episode\"\"\"\n        self.results.append(self.total_reward)\n\n    def score_episode(self, episode_num, n_episodes):\n        \"\"\"output results and save\"\"\"\n        self.save_score()\n        avglen = min(len(self.results), self.save_interval)\n        formatstr = \"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n        print(formatstr.format(time.strftime(\"%H:%M:%S\"), len(self.results),\n                               n_episodes, self.total_reward, avglen,\n                               sum(self.results[-avglen:])/avglen, memusage()),\n              end=\"\\r\", flush=False)\n\n    def run_episode(self, render=RENDER):\n        \"\"\"run a full episode\"\"\"\n        global env\n\n        self.reset()\n        self.state = env.reset()\n        self.done = False\n\n        while not self.done:\n            if render:\n                env.render()\n            self.action = self.act(self.state.reshape([1, self.state_size]))\n            self.next_state, self.reward, self.done, _ = env.step(self.action)\n            self.total_reward += self.reward\n            self.remember()\n            self.state = self.next_state\n            self.increment_time()\n\n        if render:\n            env.render()\n\n        self.train()\n\n    def rlplot(self, title='Agent Training Progress'):\n        \"\"\"plot training progress\"\"\"\n        df = pd.DataFrame({'timesteps': self.results})\n        df['avg'] = df['timesteps'].rolling(10).mean()\n\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['timesteps'],\n                                 mode='markers',\n                                 name='timesteps',\n                                 marker=dict(\n                                     color='mediumblue',\n                                     size=4,\n                                 ),\n                                ))\n\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['avg'],\n                                 mode='lines',\n                                 line_width=3,\n                                 name='moving average'))\n\n        fig.update_layout(\n            title=dict(text=title,\n                       x=0.5,\n                       xanchor='center'),\n            xaxis=dict(\n                title=\"Episodes\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            yaxis=dict(\n                title=\"Total Reward per Episode\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            legend=go.layout.Legend(\n                x=0.01,\n                y=0.99,\n                traceorder=\"normal\",\n                font=dict(\n                    family=\"sans-serif\",\n                    size=12,\n                    color=\"black\"\n                ),\n                #bgcolor=\"LightSteelBlue\",\n                bordercolor=\"Black\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.536081Z","iopub.execute_input":"2023-11-20T18:00:06.536383Z","iopub.status.idle":"2023-11-20T18:00:06.572828Z","shell.execute_reply.started":"2023-11-20T18:00:06.536359Z","shell.execute_reply":"2023-11-20T18:00:06.571960Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"RENDER = False\nOUTPUT_DIR = 'model_output/trading/'\n\nclass Agent:\n    \"\"\"abstract base class for agents\"\"\"\n\n    def __init__(self, state_size, action_size, filename=\"model\",\n                 *args, **kwargs):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.filename = filename\n        self.timestep = 0\n        self.total_reward = 0\n        self.save_interval = 10\n\n        raise NotImplementedError\n\n    def build_model(self, *args, **kwargs):\n        \"\"\"build the relevant model\"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        self.total_reward = 0\n\n    def increment_time(self):\n        \"\"\"increment timestep counter\"\"\"\n        self.timestep += 1\n\n    def remember(self, *args, **kwargs):\n        \"\"\"store the states and rewards needed to fit the model\"\"\"\n        raise NotImplementedError\n\n    def train(self, *args, **kwargs):\n        \"\"\"train the model on experience stored by remember\"\"\"\n        raise NotImplementedError\n\n    def act(self, *args, **kwargs):\n        \"\"\"pick an action using model\"\"\"\n        raise NotImplementedError\n\n    def save_score(self):\n        \"\"\"save score of each episode\"\"\"\n        self.results.append(self.total_reward)\n\n    def score_episode(self, episode_num, n_episodes):\n        \"\"\"output results and save\"\"\"\n        self.save_score()\n        avglen = min(len(self.results), self.save_interval)\n        formatstr = \"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n        print(formatstr.format(time.strftime(\"%H:%M:%S\"), len(self.results),\n                               n_episodes, self.total_reward, avglen,\n                               sum(self.results[-avglen:])/avglen, memusage()),\n              end=\"\\r\", flush=False)\n\n    def run_episode(self, render=RENDER):\n        \"\"\"run a full episode\"\"\"\n        global env\n\n        self.reset()\n        self.state = env.reset()\n        self.done = False\n\n        while not self.done:\n            if render:\n                env.render()\n            self.action = self.act(self.state.reshape([1, self.state_size]))\n            self.next_state, self.reward, self.done, _ = env.step(self.action)\n            self.total_reward += self.reward\n\n            self.remember()\n            self.state = self.next_state\n            self.increment_time()\n\n        if render:\n            env.render()\n\n        self.train()\n\n    def save(self, *args, **kwargs):\n        \"\"\"save agent to disk\"\"\"\n        raise NotImplementedError\n\n    def load(*args, **kwargs):\n        \"\"\"load agent from disk\"\"\"\n        raise NotImplementedError\n\n    def view(self):\n        \"\"\"Run an episode without training, with rendering\"\"\"\n        state = env.reset()\n        state = np.reshape(state, [1, self.state_size])\n        done = False\n\n        # run an episode\n        self.timestep = 0\n        r = 0\n        while not done:\n            env.render()\n            action = self.act(state)\n            lastmarket = self.state[0, nstocks-1]\n            state, reward, done, _ = env.step(action)\n            newmarket = self.state[0, nstocks-1]\n            print(\"prev mkt: %.4f action: %d, new mkt %f, reward %f\" % (lastmarket, action, newmarket, reward))\n            r += reward\n            state = np.reshape(state, [1, self.state_size])\n            self.timestep += 1\n        env.render()\n        print(r)\n        env.close()\n        return self.timestep\n\n    def rlplot(self, title='Trading Agent Training Progress'):\n        \"\"\"plot training progress\"\"\"\n        df = pd.DataFrame({'timesteps': self.results})\n        df['avg'] = df['timesteps'].rolling(10).mean()\n\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['timesteps'],\n                                 mode='markers',\n                                 name='timesteps',\n                                 marker=dict(\n                                     color='mediumblue',\n                                     size=4,\n                                 ),\n                                ))\n\n        fig.add_trace(go.Scatter(x=df.index,\n                                 y=df['avg'],\n                                 mode='lines',\n                                 line_width=3,\n                                 name='moving average'))\n\n        fig.update_layout(\n            title=dict(text=title,\n                       x=0.5,\n                       xanchor='center'),\n            xaxis=dict(\n                title=\"Episodes\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            yaxis=dict(\n                title=\"Total Reward per Episode\",\n                linecolor='black',\n                linewidth=1,\n                mirror=True\n            ),\n            legend=go.layout.Legend(\n                x=0.01,\n                y=0.99,\n                traceorder=\"normal\",\n                font=dict(\n                    family=\"sans-serif\",\n                    size=12,\n                    color=\"black\"\n                ),\n                #bgcolor=\"LightSteelBlue\",\n                bordercolor=\"Black\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.574149Z","iopub.execute_input":"2023-11-20T18:00:06.574424Z","iopub.status.idle":"2023-11-20T18:00:06.598279Z","shell.execute_reply.started":"2023-11-20T18:00:06.574401Z","shell.execute_reply":"2023-11-20T18:00:06.597394Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class REINFORCE_Agent(Agent):\n    \"\"\"REINFORCE policy gradient method using deep Keras NN\"\"\"\n    def __init__(self, state_size=4, action_size=2, learning_rate=0.0005,\n                 discount_rate=DISCOUNT_RATE, n_hidden_layers=2, hidden_layer_size=16,\n                 activation='relu', reg_penalty=0, dropout=0, filename=\"kreinforce\",\n                 verbose=True):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.action_space = list(range(action_size))\n        self.learning_rate = learning_rate\n        self.discount_rate = discount_rate\n\n        self.n_hidden_layers = n_hidden_layers\n        self.hidden_layer_size = hidden_layer_size\n        self.activation = activation\n        self.reg_penalty = reg_penalty\n        self.dropout = dropout\n        self.verbose = verbose\n        self.filename = filename\n\n        self.train_model, self.predict_model = self.policy_model()\n        self.results = []\n        self.save_interval = 10\n        self.reset()\n\n\n    def reset(self):\n        \"\"\"reset agent for start of episode\"\"\"\n        self.timestep = 0\n        # truncate memory\n        self.state_memory = []\n        self.action_memory = []\n        self.reward_memory = []\n        self.total_reward = 0\n\n    def policy_model(self):\n        \"\"\"set up NN model for policy.\n        predict returns probs of actions to sample from.\n        train needs discounted rewards for the episode, so we define custom loss.\n        when training use train_model with custom loss and multi input of training data and rewards.\n        when predicting use predict_model with single input.\n        \"\"\"\n\n        def custom_loss(y_true, y_pred):\n            y_pred_clip = K.clip(y_pred, 1e-8, 1-1e-8)\n            log_likelihood = y_true*K.log(y_pred_clip)\n            return K.sum(-log_likelihood*discounted_rewards)\n\n        inputs = Input(shape=(self.state_size,), name=\"Input\")\n        discounted_rewards = Input(shape=(1,), name=\"Discounted_rewards\")\n        last_layer = inputs\n\n        for i in range(self.n_hidden_layers):\n            if self.verbose:\n                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n                print(formatstr % (i + 1,\n                                   self.hidden_layer_size,\n                                   self.activation,\n                                   self.reg_penalty,\n                                   self.dropout,\n                                   ))\n            # add dropout, but not on inputs, only between hidden layers\n            if i and self.dropout:\n                last_layer = Dropout(self.dropout, name=\"Dropout%02d\" % i)(last_layer)\n\n            last_layer = Dense(units=self.hidden_layer_size,\n                               activation=self.activation,\n                               kernel_initializer=glorot_uniform(),\n                               kernel_regularizer=keras.regularizers.l2(self.reg_penalty),\n                               name=\"Dense%02d\" % i)(last_layer)\n\n        outputs = Dense(self.action_size, activation='softmax', name=\"Output\")(last_layer)\n\n        train_model = Model(inputs=[inputs, discounted_rewards], outputs=[outputs])\n        train_model.compile(optimizer=Adam(lr=self.learning_rate), loss=custom_loss)\n\n        predict_model = Model(inputs=[inputs], outputs=[outputs])\n\n        if self.verbose:\n            print(predict_model.summary())\n\n        return train_model, predict_model\n\n\n    def act(self, state):\n        \"\"\"pick an action using predict_model\"\"\"\n        probabilities = self.predict_model.predict(state)\n\n        action = np.random.choice(self.action_space, p=probabilities[0])\n        prob=probabilities[0][action]\n        #print(\"probability={}\\n\".format(probabilities[0][action]))\n        prob_memory.append(prob)\n        return action\n\n    def remember(self):\n        \"\"\"at each step save state, action, reward for future training\"\"\"\n\n        self.state_memory.append(self.state)\n        self.action_memory.append(self.action)\n        self.reward_memory.append(self.reward)\n        \n\n    def train(self):\n        \"\"\"train the model on experience stored by remember\"\"\"\n        state_memory = np.array(self.state_memory)\n        state_memory = state_memory.reshape((len(self.state_memory),self.state_size))\n        action_memory = np.array(self.action_memory)\n        reward_memory = np.array(self.reward_memory)\n\n        # one-hot actions\n        actions = np.zeros([len(action_memory), self.action_size])\n        actions[np.arange(len(action_memory)), action_memory] = 1\n\n        disc_rewards = np.zeros_like(reward_memory)\n        cumulative_rewards = 0\n        for i in reversed(range(len(reward_memory))):\n            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n            disc_rewards[i] = cumulative_rewards\n\n        # standardize\n        disc_rewards -= np.mean(disc_rewards)\n        disc_rewards /= np.std(disc_rewards) if np.std(disc_rewards) > 0 else 1\n\n        # train states v. actions, (complemented by disc_rewards_std)\n        cost = self.train_model.train_on_batch([state_memory, disc_rewards], actions)\n\n        return cost\n\n    def view(self):\n        \"\"\"Run an episode without training, with rendering\"\"\"\n        state = env.reset()\n        state = np.reshape(state, [1, self.state_size])\n        done = False\n\n        # run an episode\n        self.timestep = 0\n        r = 0\n        retarray = []\n        while not done:\n            action = self.act(state)\n            lastmarket = state[0, self.state_size//2-1]\n            state, reward, done, _ = env.step(action)\n            newmarket = state[0, self.state_size//2-1]\n            print(\"prev mkt: %.4f action: %d, new mkt %.4f, reward %f\" % (lastmarket, action, newmarket, reward))\n            r += reward\n            state = np.reshape(state, [1, self.state_size])\n            self.timestep += 1\n            retarray.append((self.timestep, action, lastmarket, newmarket, reward))\n        print(r)\n        env.close()\n        return retarray\n\n    def save(self):\n        \"save agent: pickle self and use Keras native save model\"\n        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n        self.predict_model.save(\"%s_predict.h5\" % fullname)\n        # can't save / load train model due to custom loss\n        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n\n    def load(filename, memory=True):\n        \"load saved agent\"\n        self = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n        self.predict_model = load_model(\"%s_predict.h5\" % filename)\n        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(self.results),\n                                                                      len(self.memory),\n                                                                      self.epsilon))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.599452Z","iopub.execute_input":"2023-11-20T18:00:06.599764Z","iopub.status.idle":"2023-11-20T18:00:06.628975Z","shell.execute_reply.started":"2023-11-20T18:00:06.599735Z","shell.execute_reply":"2023-11-20T18:00:06.628154Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#class Target:\n#    def __init__(self,df,):\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.629993Z","iopub.execute_input":"2023-11-20T18:00:06.630246Z","iopub.status.idle":"2023-11-20T18:00:06.642013Z","shell.execute_reply.started":"2023-11-20T18:00:06.630224Z","shell.execute_reply":"2023-11-20T18:00:06.641187Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#dt=1\n#index_1=[0,6,12,18,24,30,36,42,48,54]\n#index_2=[1,7,13,19,25,31,37,43,49]\n#index_3=[2,8,14,20,26,32,38,44,50]\n#index_4=[3,9,15,21,27,33,39,45,51]\n#index_5=[4,10,16,22,28,34,40,46,52]\n#index_6=[5,11,17,23,29,35,41,47,53]\n\n#index_pick=index_5\nN_EPISODES = 1500\n#lag is fixed\nlag = 1\n#ticks_per_episode = len(index_pick)/dt-1-lag\nnstocks = 1\n\n\n#gen = user_sub(df_new,index_1,start_trend=amplifier)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.643210Z","iopub.execute_input":"2023-11-20T18:00:06.643719Z","iopub.status.idle":"2023-11-20T18:00:06.650635Z","shell.execute_reply.started":"2023-11-20T18:00:06.643689Z","shell.execute_reply":"2023-11-20T18:00:06.649930Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#def shm_market_gen():\n#    return market_gen(gen=user_sub(df_new,index_pick,start_trend=amplifier),\n#                      lag=lag)\n\n\n#gen=shm_market_gen()\n#time_series=[]\n#stock_series=[]\n#for i in range(len(index_pick)-lag):\n#    z = next(gen)\n#    time_series.append(i)\n#    stock_series.append(z[1])\n\n#df_gen = pd.DataFrame({'dateindex': time_series, 'stock': stock_series})\n\n#make_figure(df_gen['dateindex'], df_gen['stock'],\n#            title='Genarated market',\n#            xtitle='Timesteps',\n#            ytitle='buffer+diffbuffer'\n#           )","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.651826Z","iopub.execute_input":"2023-11-20T18:00:06.652190Z","iopub.status.idle":"2023-11-20T18:00:06.663178Z","shell.execute_reply.started":"2023-11-20T18:00:06.652161Z","shell.execute_reply":"2023-11-20T18:00:06.662310Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"result_index=[]\n\nfor i in range(len(index_list)):\n    ticks_per_episode = len(index_list[i])/dt-1-lag\n    def shm_market_gen():\n        return market_gen(gen=user_sub(df_index,index_list[i],start_trend=amplifier),\n                      lag=1)\n\n    env = Market(shm_market_gen,\n             lag=lag,\n             nstocks=1,\n             episode_length=ticks_per_episode)\n\n    agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n                        action_size=3,\n                       )\n\n    start_time = time.time()\n    print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n    for e in range(N_EPISODES):\n        agent.run_episode()\n        agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n    elapsed_time = time.time() - start_time\n    print(\"\\nTrain time: \", elapsed_time)\n    env.reset()\n    z = agent.view()\n\n    df = pd.DataFrame(z)\n    df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n    df['lastmarket']+=1\n    df['newmarket']+=1\n    df['short'] = np.nan\n    df.loc[df['action']==0, 'short'] = df['newmarket']\n    df['flat'] = np.nan\n    df.loc[df['action']==1, 'flat'] = df['newmarket']\n    df['long'] = np.nan\n    df.loc[df['action']==2, 'long'] = df['newmarket']\n    df['totalreward'] = df['reward'].cumsum()\n    df.to_csv('df_index_list_{}.csv'.format(i))\n    prob=agent.predict_model.predict(agent.state_memory)\n    action = np.random.choice(agent.action_space, p=prob[0])\n    arr=np.array(df_index['stock'][index_list[i]])\n    mean_diff=np.mean(np.absolute(np.diff(arr)))\n    action_arr=np.array(df['action']-1)\n    pick_index=np.array(df_index['stock'][index_list[i]])\n    wap_future_index=[]\n    #assume in the first time index of t+60 the same as t\n    wap_future_index.append(pick_index[0])\n    for j in range(len(action_arr)):\n        wap_future_index.append(action_arr[j]*mean_diff+pick_index[j+1])\n    wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n    seconds_in_bucket=[j * 10 for j in index_list[i]]\n    comb=list(zip(seconds_in_bucket,pick_index,wap_future_index))\n    result_index.append(comb)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T18:00:06.664312Z","iopub.execute_input":"2023-11-20T18:00:06.664539Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"layer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 18:00:06\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"18:00:38 episode 1500/1500:, score: 2.2502900000000636, 10-episode avg: 1.6 Memory: 2.3 MB            \nTrain time:  32.064544677734375\nprev mkt: 1.0869 action: 2, new mkt 1.3753, reward 0.288445\nprev mkt: 1.3753 action: 0, new mkt 0.7831, reward 0.592189\nprev mkt: 0.7831 action: 2, new mkt 1.1325, reward 0.349364\nprev mkt: 1.1325 action: 0, new mkt 0.4120, reward 0.720463\nprev mkt: 0.4120 action: 0, new mkt 0.4070, reward 0.005050\nprev mkt: 0.4070 action: 2, new mkt 0.8868, reward 0.479815\nprev mkt: 0.8868 action: 0, new mkt 1.1438, reward -0.256987\nprev mkt: 1.1438 action: 0, new mkt 1.4106, reward -0.266805\n1.9115339999999605\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\nStart: 18:00:39\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"18:01:02 episode 1500/1500:, score: 2.1343600000002425, 10-episode avg: 2.1 Memory: 2.3 MB           \nTrain time:  23.259067058563232\nprev mkt: 1.4387 action: 2, new mkt 1.4476, reward 0.008884\nprev mkt: 1.4476 action: 0, new mkt 0.9719, reward 0.475623\nprev mkt: 0.9719 action: 2, new mkt 1.0910, reward 0.119049\nprev mkt: 1.0910 action: 0, new mkt 0.2583, reward 0.832733\nprev mkt: 0.2583 action: 2, new mkt 0.6808, reward 0.422542\nprev mkt: 0.6808 action: 2, new mkt 1.0839, reward 0.403097\nprev mkt: 1.0839 action: 0, new mkt 1.2115, reward -0.127568\n2.1343600000002425\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"None\nStart: 18:01:02\n18:01:26 episode 1500/1500:, score: 1.9326729999999088, 10-episode avg: 1.5 Memory: 2.3 MB           \nTrain time:  23.72875475883484\nprev mkt: 1.6056 action: 0, new mkt 1.4413, reward 0.164294\nprev mkt: 1.4413 action: 0, new mkt 0.9639, reward 0.477411\nprev mkt: 0.9639 action: 0, new mkt 0.9588, reward 0.005050\nprev mkt: 0.9588 action: 0, new mkt 0.1924, reward 0.766411\nprev mkt: 0.1924 action: 2, new mkt 0.6607, reward 0.468280\nprev mkt: 0.6607 action: 2, new mkt 1.1868, reward 0.526152\nprev mkt: 1.1868 action: 1, new mkt 1.2381, reward 0.000000\n2.407597999999439\nlayer 1 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nlayer 2 size 16, relu, reg_penalty 0.00000000, dropout 0.000\nModel: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 2)]               0         \n                                                                 \n Dense00 (Dense)             (None, 16)                48        \n                                                                 \n Dense01 (Dense)             (None, 16)                272       \n                                                                 \n Output (Dense)              (None, 3)                 51        \n                                                                 \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning:\n\nThe `lr` argument is deprecated, use `learning_rate` instead.\n\n/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning:\n\n`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n\n","output_type":"stream"},{"name":"stdout","text":"None\nStart: 18:01:26\n18:01:31 episode 300/1500:, score: -0.4341970000001538, 10-episode avg: 0.2 Memory: 2.3 MB             \r","output_type":"stream"}]},{"cell_type":"code","source":"from itertools import chain\n\nresult_indx_list = list(chain.from_iterable(result_index))\n#print(result_indx_list)\nsorted_indx_result = sorted(result_indx_list, key=lambda x: x[0])\n#print(sorted_indx_result)\nseconds_in_bucket, wap_index, wap_index_future = zip(*sorted_indx_result)\n#seconds_in_bucket_list=list(seconds_in_bucket)\nwap_index_list=list(wap_index)\nwap_index_future_list=list(wap_index_future)\nprint(list(seconds_in_bucket))\nprint(list(wap_index))\nprint(list(wap_index_future))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=[]\n\nfor i in range(len(index_list)):\n    ticks_per_episode = len(index_list[i])/dt-1-lag\n    def shm_market_gen():\n        return market_gen(gen=user_sub(df_new,index_list[i],start_trend=amplifier),\n                      lag=1)\n\n    env = Market(shm_market_gen,\n             lag=lag,\n             nstocks=1,\n             episode_length=ticks_per_episode)\n\n    agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n                        action_size=3,\n                       )\n\n    start_time = time.time()\n    print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n    for e in range(N_EPISODES):\n        agent.run_episode()\n        agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n    elapsed_time = time.time() - start_time\n    print(\"\\nTrain time: \", elapsed_time)\n    env.reset()\n    z = agent.view()\n\n    df = pd.DataFrame(z)\n    df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n    df['lastmarket']+=1\n    df['newmarket']+=1\n    df['short'] = np.nan\n    df.loc[df['action']==0, 'short'] = df['newmarket']\n    df['flat'] = np.nan\n    df.loc[df['action']==1, 'flat'] = df['newmarket']\n    df['long'] = np.nan\n    df.loc[df['action']==2, 'long'] = df['newmarket']\n    df['totalreward'] = df['reward'].cumsum()\n    df.to_csv('df_list_{}.csv'.format(i))\n    prob=agent.predict_model.predict(agent.state_memory)\n    action = np.random.choice(agent.action_space, p=prob[0])\n    pick_index=np.array(df_new['stock'][index_list[i]])\n    mean_diff=np.mean(np.absolute(np.diff(arr)))\n    action_arr=np.array(df['action']-1)\n    #pick_index=np.array(df_new['stock'][index_list[i]])\n    wap_future_index=[]\n    #assume in the first time index of t+60 the same as t\n    wap_future_index.append(pick_index[0])\n    for j in range(len(action_arr)):\n        wap_future_index.append(action_arr[j]*mean_diff+pick_index[j+1])\n    wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n    seconds_in_bucket=[j * 10 for j in index_list[i]]\n    comb=list(zip(seconds_in_bucket,pick_index,wap_future_index))\n    result.append(comb)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresult_list = list(chain.from_iterable(result))\n#print(result_list)\nsorted_result = sorted(result_list, key=lambda x: x[0])\n#print(sorted_result)\nseconds_in_bucket, wap, wap_future = zip(*sorted_result)\nseconds_in_bucket_list=list(seconds_in_bucket)\nwap_list=list(wap)\nwap_future_list=list(wap_future)\nprint(list(seconds_in_bucket))\nprint(list(wap))\nprint(list(wap_future))\ntarget_out=[]\nfor i in range(len(seconds_in_bucket_list)):\n    target_out.append((wap_future_list[i]/wap_list[i]-wap_index_future_list[i]/wap_index_list[i])*10000)\n\nprint(target_out)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excelfilereader=pd.read_csv(r'/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\ndata_sub=excelfilereader[excelfilereader['stock_id']==stock]\ndata_sub2=data_sub[data_sub['revealed_date_id']==date]\ndata=pd.DataFrame(data_sub2,columns=['revealed_target'])\ndata = data.values.tolist()\ndata=np.squeeze(data)\n#print(data)\n\nplt.plot(data,label=\"truth\")\nplt.plot(target_out,label=\"Calculated\")\nplt.ylabel('Target')\nplt.title('Target of stock {} at date {}'.format(stock,date))\nplt.legend()\nplt.show()\n\nfrom sklearn.metrics import mean_absolute_error as mae\nerror = mae(data, target_out) \n\nprint(\"Mean absolute error : \" + str(error)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#agent.rlplot(\"Training Progress: stock {} date {}\".format(int(stock),int(date)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#env.reset()\n#z = agent.view()\n\n#df = pd.DataFrame(z)\n#df.columns = [\"timestep\", \"action\", \"lastmarket\", \"newmarket\", \"reward\"]\n#df['lastmarket']+=1\n#df['newmarket']+=1\n#df['short'] = np.nan\n#df.loc[df['action']==0, 'short'] = df['newmarket']\n#df['flat'] = np.nan\n#df.loc[df['action']==1, 'flat'] = df['newmarket']\n#df['long'] = np.nan\n#df.loc[df['action']==2, 'long'] = df['newmarket']\n#df['totalreward'] = df['reward'].cumsum()\n#df.to_csv('df.csv')\n#df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def tradesim_chart(df, title=\"Trading Simulation\"):\n\n#    fig = go.Figure()\n#    markersize=4\n\n    # x axis\n#    x = df['timestep']\n\n#    red = 'rgba(192, 32, 32, 0.75)'\n#    blue = 'rgba(32, 32, 192, 0.75)'\n#    green = 'rgba(0, 204, 0, 0.75)'\n#    black = 'rgba(32, 32, 32, 0.75)'\n\n#    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n#    fig.add_trace(go.Scatter(y=df['short'],\n#                             x=x,\n#                             name='Short (left axis)',\n#                             mode='markers',\n#                             marker=dict(size=markersize,\n#                                         color=red),\n#                            ),\n #                 secondary_y=False,\n #                )\n\n  #  fig.add_trace(go.Scatter(y=df['flat'],\n  #                           x=x,\n  #                           name='Flat (left axis)',\n  #                           mode='markers',\n  #                           marker=dict(size=markersize,\n  #                                       color=blue),\n  #                          ),\n  #                secondary_y=False,\n  #               )\n\n  #  fig.add_trace(go.Scatter(y=df['long'],\n#                          x=x,\n#                             name='Long (left axis)',\n#                             mode='markers',\n#                             marker=dict(size=markersize,\n#                                         color=green),\n#                            ),\n#                  secondary_y=False,\n#                 )\n\n #   fig.add_trace(go.Scatter(y=df['totalreward'],\n #                            x=x,\n #                            name='Total reward (right)',\n  #                           mode='markers',\n  #                           marker=dict(size=markersize,\n  #                                       color=black),\n  #                          ),\n  #                secondary_y=True,\n  #               )\n\n    # plot attributes\n  #  fig.update_layout(\n  #      title= dict(text=title,\n  #                  x=0.5,\n  #                  xanchor='center'),\n  #      xaxis=dict(\n  #          title=\"Timesteps\",\n  #          linecolor='black',\n  #          linewidth=1,\n  #          mirror=True\n  #      ),\n  #      yaxis=dict(\n  #          title=\"Price\",\n  #          linecolor='black',\n  #          linewidth=1,\n  #          mirror=True\n  #      ),\n  #      showlegend=True,\n  #      legend=dict(x=0.738, y=0.05)\n  #  )\n\n  #  fig.update_yaxes(title_text=\"Total reward\", secondary_y=True)\n\n  #  fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tradesim_chart(df, title=\"Trading stock {} date {}\".format(int(stock),int(date)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prob=agent.predict_model.predict(agent.state_memory)\n#print(prob)\n#action = np.random.choice(agent.action_space, p=prob[0])\n#print(action)\n#action\n#arr=np.array(df_new['stock'][index_pick])\n#mean_diff=np.mean(np.absolute(np.diff(arr)))\n#action_arr=np.array(df['action']-1)\n#print(action_arr)\n#pick=np.array(df_new['stock'][index_pick])\n#wap_future=[]\n#assume the first stock i\n#wap_future.append(pick[0])\n#for i in range(len(action_arr)):\n#    wap_future.append(action_arr[i]*mean_diff+pick[i+1])\n#index=[1,2,3,4,5,6,7,8,9]\n#wap_future.append((action-1)*mean_diff+pick[-1])\n#plt.plot(pick,\"*\",label=\"truth\")\n#plt.plot(index,wap_future,\"*\",label=\"prediction\")\n#plt.legend()\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def shm_market_gen():\n#    return market_gen(gen=user_sub(df_index,index_pick,start_trend=amplifier),\n#                      lag=lag)\n\n#env = Market(shm_market_gen,\n#             lag=lag,\n#             nstocks=1,\n#             episode_length=ticks_per_episode)\n\n#agent = REINFORCE_Agent(state_size=nstocks*lag*2,\n#                        action_size=3,\n#                       )\n#agent.reset()\n#start_time = time.time()\n#print(\"Start: %s\" % (time.strftime(\"%H:%M:%S\")))\n\n#for e in range(N_EPISODES):\n#    agent.run_episode()\n#    agent.score_episode(e, N_EPISODES)\n    \n\n    #if e and (e+1) % agent.save_interval == 0:\n    #    agent.save()\n\n#elapsed_time = time.time() - start_time\n#print(\"\\nTrain time: \", elapsed_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#agent.rlplot(\"Training Progress: Index date {}\".format(int(date)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prob=agent.predict_model.predict(agent.state_memory)\n#print(prob)\n#action = np.random.choice(agent.action_space, p=prob[0])\n#print(action)\n#action\n#arr=np.array(df_index['stock'][index_pick])\n#mean_diff=np.mean(np.absolute(np.diff(arr)))\n#print(np.mean(np.absolute(np.diff(arr))))\n#action_arr=np.array(df['action']-1)\n#print(action_arr)\n#pick_index=np.array(df_index['stock'][index_pick])\n#wap_future_index=[]\n#assume in the first time index of t+60 the same as t\n#wap_future_index.append(pick_index[0])\n#for i in range(len(action_arr)):\n#    wap_future_index.append(action_arr[i]*mean_diff+pick_index[i+1])\n#index=[1,2,3,4,5,6,7,8,9]\n#wap_future_index.append((action-1)*mean_diff+pick_index[-1])\n#plt.plot(pick_index,\"*\",label=\"truth\")\n#plt.plot(index,wap_future_index,\"*\",label=\"prediction\")\n#plt.legend()\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def target():\n#    target_out=[]\n#    for i in range(len(index_pick)):\n#        target_out.append((wap_future[i]/pick[i]-wap_future_index[i]/pick_index[i])*10000)\n#    return target_out\n\n#seconds_in_bucket=[i * 10 for i in index_pick]\n#comb=list(zip(seconds_in_bucket,target()))\n#print(\"day=\",date,\"Stock=\",stock,\"seconds_in_bucket\", seconds_in_bucket, target())\n#print(comb)        \n#sorted_zipped = sorted(comb, key=lambda x: x[0])    \n#sorted_list1, sorted_list2 = zip(*sorted_zipped)\n#print(list(sorted_list1))\n#print(list(sorted_list2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#env = Market(shm_market_gen,\n#             lag=lag,\n#             nstocks=1,\n#             episode_length=ticks_per_episode)\n#env.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import optiver2023\n#env = optiver2023.make_env()\n#iter_test = env.iter_test()\n#counter = 0\n#for (test, revealed_targets, sample_prediction) in iter_test:\n    #feat = generate_features(test)\n    \n    #sample_prediction['target'] \n#    env.predict(sample_prediction)\n#    counter += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}